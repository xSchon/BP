{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7144438c-d700-42a8-be1b-bb1fe3248171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from feature_engine.creation import CyclicalTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from catboost import Pool\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db252221-836d-4a91-a70d-39b81218dba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/data_after_preprocessing.csv'  # path to the input data\n",
    "\n",
    "END_DATE = '04/24/2022'                          # the last day we have the data for\n",
    "\n",
    "DATA_CLUSTERS_PATH = 'data/clusters/'            # path to csv files generated by this notebook\n",
    "\n",
    "# Full generated file name is NORMALIZED(STANDARD)_FILE_NAME_kmeans-{kmeans_cluster_id}_hier-{hierarchical_cluster_id}.csv\n",
    "NORMALIZED_FILE_NAME = 'normalized_prepared'     # unified csv file name for weekly sales in normalized form\n",
    "STANDARD_FILE_NAME = 'standard_prepared'         # unified csv file name for weekly sales in normalized form\n",
    "\n",
    "# Check whether there is folder for clusters saving\n",
    "if not os.path.exists(DATA_CLUSTERS_PATH):\n",
    "    os.makedirs(DATA_CLUSTERS_PATH)\n",
    "\n",
    "# Display all of the columns when data are shown\n",
    "pd.set_option('display.max_columns', 60)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16, 8)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "# In some parts of this notebook there are references to \"time run on laptop\".\n",
    "# Laptop specifications - OS: Ubuntu 21.04 LTS\n",
    "#                         RAM: 16GB\n",
    "#                         CPU: Intel(R) Core(TM) i7-8750H CPU @ 2.20GHz\n",
    "#                         GPU: GeForce GTX 1050 \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c13f8c1d-9a79-4d38-a319-0029ad1d5419",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  pd.read_csv(DATA_PATH, sep=',', parse_dates=['doc_date', 'product_since'], low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a450e01-7167-4208-a808-435bfb09ebee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill_country</th>\n",
       "      <th>currency_id</th>\n",
       "      <th>basket_id</th>\n",
       "      <th>doc_date</th>\n",
       "      <th>exchange_currency_rate</th>\n",
       "      <th>basket_total_price_with_vat</th>\n",
       "      <th>count_basket_items</th>\n",
       "      <th>basket_count_products</th>\n",
       "      <th>basket_type</th>\n",
       "      <th>item_quantity</th>\n",
       "      <th>item_type</th>\n",
       "      <th>item_unit_price_with_vat</th>\n",
       "      <th>item_total_discount_with_vat</th>\n",
       "      <th>product_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>brand_id</th>\n",
       "      <th>product_status</th>\n",
       "      <th>reviews_count</th>\n",
       "      <th>reviews_average_score_price</th>\n",
       "      <th>reviews_average_score_quality</th>\n",
       "      <th>reviews_average_score_properties</th>\n",
       "      <th>reviews_average_score_overall</th>\n",
       "      <th>reviews_average_score</th>\n",
       "      <th>is_in_stock</th>\n",
       "      <th>is_ended</th>\n",
       "      <th>is_new</th>\n",
       "      <th>is_boosted</th>\n",
       "      <th>product_purchase_price</th>\n",
       "      <th>eshop_stock_count</th>\n",
       "      <th>is_fifo</th>\n",
       "      <th>product_since</th>\n",
       "      <th>category_status</th>\n",
       "      <th>segment_id</th>\n",
       "      <th>default_warranty_period</th>\n",
       "      <th>doc_day</th>\n",
       "      <th>doc_month</th>\n",
       "      <th>doc_year</th>\n",
       "      <th>doc_weekday</th>\n",
       "      <th>doc_week</th>\n",
       "      <th>days_in_shop</th>\n",
       "      <th>ancestor_count</th>\n",
       "      <th>descendant_count</th>\n",
       "      <th>cluster_hierar</th>\n",
       "      <th>kmeans_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1136409</td>\n",
       "      <td>2020-04-26</td>\n",
       "      <td>1.9558</td>\n",
       "      <td>345.00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.34</td>\n",
       "      <td>0.28</td>\n",
       "      <td>39848</td>\n",
       "      <td>179.0</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>83.0</td>\n",
       "      <td>95.192308</td>\n",
       "      <td>93.653846</td>\n",
       "      <td>93.653846</td>\n",
       "      <td>94.423077</td>\n",
       "      <td>94.277108</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2050</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-22</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>2020</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1571607</td>\n",
       "      <td>2020-08-07</td>\n",
       "      <td>1.9558</td>\n",
       "      <td>319.00</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>39848</td>\n",
       "      <td>179.0</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>83.0</td>\n",
       "      <td>95.192308</td>\n",
       "      <td>93.653846</td>\n",
       "      <td>93.653846</td>\n",
       "      <td>94.423077</td>\n",
       "      <td>94.277108</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2050</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-22</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1661756</td>\n",
       "      <td>2020-08-29</td>\n",
       "      <td>1.9558</td>\n",
       "      <td>180.95</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>39848</td>\n",
       "      <td>179.0</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>83.0</td>\n",
       "      <td>95.192308</td>\n",
       "      <td>93.653846</td>\n",
       "      <td>93.653846</td>\n",
       "      <td>94.423077</td>\n",
       "      <td>94.277108</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2050</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-22</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>29</td>\n",
       "      <td>8</td>\n",
       "      <td>2020</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1701910</td>\n",
       "      <td>2020-09-09</td>\n",
       "      <td>1.9558</td>\n",
       "      <td>127.31</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>39848</td>\n",
       "      <td>179.0</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>83.0</td>\n",
       "      <td>95.192308</td>\n",
       "      <td>93.653846</td>\n",
       "      <td>93.653846</td>\n",
       "      <td>94.423077</td>\n",
       "      <td>94.277108</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2050</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-22</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>262709</td>\n",
       "      <td>2019-06-06</td>\n",
       "      <td>1.9558</td>\n",
       "      <td>240.57</td>\n",
       "      <td>8.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>39848</td>\n",
       "      <td>179.0</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>83.0</td>\n",
       "      <td>95.192308</td>\n",
       "      <td>93.653846</td>\n",
       "      <td>93.653846</td>\n",
       "      <td>94.423077</td>\n",
       "      <td>94.277108</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2050</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-22</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2713505</th>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>3375295</td>\n",
       "      <td>2021-08-16</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>70.51</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>62.62</td>\n",
       "      <td>56.38</td>\n",
       "      <td>272147</td>\n",
       "      <td>2854.0</td>\n",
       "      <td>6755</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.7900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-05-04</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>355</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2713506</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3057913</td>\n",
       "      <td>2022-02-03</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>57.30</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>28.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>301637</td>\n",
       "      <td>1296.0</td>\n",
       "      <td>6481</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.6300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-11-16</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>159</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2713507</th>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>3325485</td>\n",
       "      <td>2021-08-04</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>329.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>329.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>274590</td>\n",
       "      <td>1304.0</td>\n",
       "      <td>6772</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>175.3200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-05-25</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>334</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2713508</th>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>1728853</td>\n",
       "      <td>2020-09-17</td>\n",
       "      <td>7.5415</td>\n",
       "      <td>260.96</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>16.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>191250</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>2799</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.9475</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>870</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2713509</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4623935</td>\n",
       "      <td>2022-04-08</td>\n",
       "      <td>24.5120</td>\n",
       "      <td>69.35</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.40</td>\n",
       "      <td>3.26</td>\n",
       "      <td>203608</td>\n",
       "      <td>1068.0</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>772</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2713510 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         bill_country  currency_id  basket_id   doc_date  \\\n",
       "0                   1            1    1136409 2020-04-26   \n",
       "1                   1            1    1571607 2020-08-07   \n",
       "2                   1            1    1661756 2020-08-29   \n",
       "3                   1            1    1701910 2020-09-09   \n",
       "4                   1            1     262709 2019-06-06   \n",
       "...               ...          ...        ...        ...   \n",
       "2713505            12            6    3375295 2021-08-16   \n",
       "2713506             3            6    3057913 2022-02-03   \n",
       "2713507             9            6    3325485 2021-08-04   \n",
       "2713508            11            9    1728853 2020-09-17   \n",
       "2713509             2            4    4623935 2022-04-08   \n",
       "\n",
       "         exchange_currency_rate  basket_total_price_with_vat  \\\n",
       "0                        1.9558                       345.00   \n",
       "1                        1.9558                       319.00   \n",
       "2                        1.9558                       180.95   \n",
       "3                        1.9558                       127.31   \n",
       "4                        1.9558                       240.57   \n",
       "...                         ...                          ...   \n",
       "2713505                  1.0000                        70.51   \n",
       "2713506                  1.0000                        57.30   \n",
       "2713507                  1.0000                       329.00   \n",
       "2713508                  7.5415                       260.96   \n",
       "2713509                 24.5120                        69.35   \n",
       "\n",
       "         count_basket_items  basket_count_products  basket_type  \\\n",
       "0                      18.0                   18.0            1   \n",
       "1                      11.0                   13.0            1   \n",
       "2                       2.0                    2.0            1   \n",
       "3                       2.0                    2.0            1   \n",
       "4                       8.0                   28.0            1   \n",
       "...                     ...                    ...          ...   \n",
       "2713505                 2.0                    2.0            1   \n",
       "2713506                 3.0                    3.0            1   \n",
       "2713507                 1.0                    1.0            3   \n",
       "2713508                14.0                   14.0            1   \n",
       "2713509                 7.0                   10.0            1   \n",
       "\n",
       "         item_quantity  item_type  item_unit_price_with_vat  \\\n",
       "0                  1.0          1                      5.34   \n",
       "1                  1.0          1                      5.62   \n",
       "2                  1.0          1                      5.62   \n",
       "3                  1.0          1                      5.62   \n",
       "4                  3.0          1                      5.11   \n",
       "...                ...        ...                       ...   \n",
       "2713505            1.0          1                     62.62   \n",
       "2713506            1.0          1                     28.90   \n",
       "2713507            1.0          1                    329.00   \n",
       "2713508            1.0          1                     16.84   \n",
       "2713509            1.0          1                     20.40   \n",
       "\n",
       "         item_total_discount_with_vat  product_id  category_id  brand_id  \\\n",
       "0                                0.28       39848        179.0       145   \n",
       "1                                0.00       39848        179.0       145   \n",
       "2                                0.00       39848        179.0       145   \n",
       "3                                0.00       39848        179.0       145   \n",
       "4                                0.00       39848        179.0       145   \n",
       "...                               ...         ...          ...       ...   \n",
       "2713505                         56.38      272147       2854.0      6755   \n",
       "2713506                          0.00      301637       1296.0      6481   \n",
       "2713507                          0.00      274590       1304.0      6772   \n",
       "2713508                          0.00      191250       2042.0      2799   \n",
       "2713509                          3.26      203608       1068.0        34   \n",
       "\n",
       "         product_status  reviews_count  reviews_average_score_price  \\\n",
       "0                     1           83.0                    95.192308   \n",
       "1                     1           83.0                    95.192308   \n",
       "2                     1           83.0                    95.192308   \n",
       "3                     1           83.0                    95.192308   \n",
       "4                     1           83.0                    95.192308   \n",
       "...                 ...            ...                          ...   \n",
       "2713505               1            0.0                     0.000000   \n",
       "2713506               1            1.0                     0.000000   \n",
       "2713507               3            0.0                     0.000000   \n",
       "2713508               1            0.0                     0.000000   \n",
       "2713509               1            0.0                     0.000000   \n",
       "\n",
       "         reviews_average_score_quality  reviews_average_score_properties  \\\n",
       "0                            93.653846                         93.653846   \n",
       "1                            93.653846                         93.653846   \n",
       "2                            93.653846                         93.653846   \n",
       "3                            93.653846                         93.653846   \n",
       "4                            93.653846                         93.653846   \n",
       "...                                ...                               ...   \n",
       "2713505                       0.000000                          0.000000   \n",
       "2713506                       0.000000                          0.000000   \n",
       "2713507                       0.000000                          0.000000   \n",
       "2713508                       0.000000                          0.000000   \n",
       "2713509                       0.000000                          0.000000   \n",
       "\n",
       "         reviews_average_score_overall  reviews_average_score  is_in_stock  \\\n",
       "0                            94.423077              94.277108            1   \n",
       "1                            94.423077              94.277108            1   \n",
       "2                            94.423077              94.277108            1   \n",
       "3                            94.423077              94.277108            1   \n",
       "4                            94.423077              94.277108            1   \n",
       "...                                ...                    ...          ...   \n",
       "2713505                       0.000000               0.000000            1   \n",
       "2713506                       0.000000             100.000000            1   \n",
       "2713507                       0.000000               0.000000            0   \n",
       "2713508                       0.000000               0.000000            1   \n",
       "2713509                       0.000000               0.000000            0   \n",
       "\n",
       "         is_ended  is_new  is_boosted  product_purchase_price  \\\n",
       "0               0       0           0                  2.2050   \n",
       "1               0       0           0                  2.2050   \n",
       "2               0       0           0                  2.2050   \n",
       "3               0       0           0                  2.2050   \n",
       "4               0       0           0                  2.2050   \n",
       "...           ...     ...         ...                     ...   \n",
       "2713505         1       0           0                 29.7900   \n",
       "2713506         0       0           0                 12.6300   \n",
       "2713507         1       0           0                175.3200   \n",
       "2713508         0       0           0                 10.9475   \n",
       "2713509         0       0           0                 12.0000   \n",
       "\n",
       "         eshop_stock_count  is_fifo product_since  category_status  \\\n",
       "0                     -2.0        0    2019-01-22                1   \n",
       "1                     -2.0        0    2019-01-22                1   \n",
       "2                     -2.0        0    2019-01-22                1   \n",
       "3                     -2.0        0    2019-01-22                1   \n",
       "4                     -2.0        0    2019-01-22                1   \n",
       "...                    ...      ...           ...              ...   \n",
       "2713505                1.0        0    2021-05-04                1   \n",
       "2713506                1.0        0    2021-11-16                1   \n",
       "2713507                0.0        0    2021-05-25                1   \n",
       "2713508                4.0        0    2019-12-06                1   \n",
       "2713509                0.0        0    2020-03-13                1   \n",
       "\n",
       "         segment_id  default_warranty_period  doc_day  doc_month  doc_year  \\\n",
       "0               1.0                     24.0       26          4      2020   \n",
       "1               1.0                     24.0        7          8      2020   \n",
       "2               1.0                     24.0       29          8      2020   \n",
       "3               1.0                     24.0        9          9      2020   \n",
       "4               1.0                     24.0        6          6      2019   \n",
       "...             ...                      ...      ...        ...       ...   \n",
       "2713505         6.0                     24.0       16          8      2021   \n",
       "2713506         3.0                     24.0        3          2      2022   \n",
       "2713507         3.0                     24.0        4          8      2021   \n",
       "2713508        16.0                     24.0       17          9      2020   \n",
       "2713509         5.0                     24.0        8          4      2022   \n",
       "\n",
       "         doc_weekday  doc_week  days_in_shop  ancestor_count  \\\n",
       "0                  6        17          1188               3   \n",
       "1                  4        32          1188               3   \n",
       "2                  5        35          1188               3   \n",
       "3                  2        37          1188               3   \n",
       "4                  3        23          1188               3   \n",
       "...              ...       ...           ...             ...   \n",
       "2713505            0        33           355               4   \n",
       "2713506            3         5           159               2   \n",
       "2713507            2        31           334               2   \n",
       "2713508            3        38           870               1   \n",
       "2713509            4        14           772               4   \n",
       "\n",
       "         descendant_count  cluster_hierar  kmeans_cluster  \n",
       "0                     1.0               1              37  \n",
       "1                     1.0               1              37  \n",
       "2                     1.0               1              37  \n",
       "3                     1.0               1              37  \n",
       "4                     1.0               1              37  \n",
       "...                   ...             ...             ...  \n",
       "2713505               1.0               1              26  \n",
       "2713506               1.0               3              68  \n",
       "2713507               1.0               3              26  \n",
       "2713508               1.0               2              42  \n",
       "2713509               1.0               3              98  \n",
       "\n",
       "[2713510 rows x 44 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55ffeda-77d1-4c43-a5fc-83a2c37da423",
   "metadata": {},
   "source": [
    "We want to make predictions weekly, as there are data with too little density for good daily prediction. Also we don't need to predict every single day, every week is fully enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c16a930d-602e-4ec4-95f5-a5ab30b4d6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero values would be in 2.34% of days\n"
     ]
    }
   ],
   "source": [
    "days_together = data[['product_id', 'days_in_shop']].drop_duplicates().days_in_shop.sum()\n",
    "all_sales_amount = len(data.index)\n",
    "print(f'Non-zero values would be in {round(all_sales_amount / days_together, 5)*100}% of days')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc331e6c-5160-4185-85a5-3ea1a5039898",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e697db8-e351-42e6-b4bc-b114f8195c78",
   "metadata": {},
   "source": [
    "## Drop unsettable columns\n",
    "We want to predict, how many items of given type we will sell. That means - the predicted column will be 'item_quantity'. We don't know, how many items will be in basket, it's type etc. \\\n",
    "We only know values of columns we can influence (or which are already written) - for example price of the product, or it's brand. \\\n",
    "These columns were great for clustering products, as they can show some patterns in them, but for pure pretictions we are dropping those data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "246f0525-6d80-4cfb-95c3-306d078ecf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(labels=['basket_id', 'basket_total_price_with_vat', 'basket_count_products', 'basket_type', 'count_basket_items', 'bill_country', 'currency_id', 'exchange_currency_rate', 'is_in_stock', 'eshop_stock_count'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10cad69-41a4-44b8-97a9-55f7bd8d847a",
   "metadata": {},
   "source": [
    "After using Variance Threshold selection to find the columns with small variance we've found that LOW_VARIANCE_COLUMNS are viable for dropping as they are not good features for models. \\\n",
    "Code for finding such columns is provided and commented, but it is not desired to be a part of every program run, as this calculation takes some time. \\\n",
    "If use desires though, they can test the code, as written, on multiple clusters and find if the LOW_VARIANCE_COLUMNS list is still accurate. \\\n",
    "Those columns are dropped at this place though, so if such tests are desired it is necessary to remove the row 'data.drop(labels=LOW_VARIANCE_COLUMNS, axis=1)'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e07f853a-1a9d-4a3f-bc41-4ca994428e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop columns that have too small variance to influence models \n",
    "# sel = VarianceThreshold(threshold=(0.16))\n",
    "# sel.fit_transform(selected_cluster)\n",
    "# to_drop = list(set(selected_cluster.columns) - set(sel.get_feature_names_out(selected_cluster.columns)))\n",
    "\n",
    "LOW_VARIANCE_COLUMNS = ['descendant_count',\n",
    " 'item_type',\n",
    " 'is_ended',\n",
    " 'is_fifo',\n",
    " 'category_status',\n",
    " 'is_new',\n",
    " 'is_boosted']\n",
    "\n",
    "data = data.drop(labels=LOW_VARIANCE_COLUMNS, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "714167fb-1e41-4399-9bdf-3f9689fdd645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_values(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transform columns by transformer suitable for them.\n",
    "    Params\n",
    "        df -> DataFrame containing columns for transformation\n",
    "    Returns\n",
    "        DataFrame -> DataFrame with values normalized by different techniques\n",
    "    \"\"\"\n",
    "\n",
    "    # Cyclical transformation of date variables\n",
    "    cyclical = CyclicalTransformer(variables=['week', 'month'], drop_original=True)\n",
    "    df = cyclical.fit_transform(df)\n",
    "    \n",
    "    \n",
    "    # Normalization for some variables is the best solution\n",
    "    df['year'] = df['year'] / max(df['year'])\n",
    "\n",
    "    \n",
    "    # Use min max scaling for remaining columns\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_columns =['item_unit_price_with_vat', 'product_purchase_price', 'item_total_discount_with_vat', 'product_id', 'category_id', 'brand_id', 'segment_id', 'reviews_count',\n",
    "                     'reviews_average_score_price', 'reviews_average_score_quality', 'reviews_average_score_properties', 'reviews_average_score_overall', 'reviews_average_score',\n",
    "                     'product_status', 'ancestor_count', 'days_in_shop']\n",
    "    \n",
    "    for previous in range (1, PAST_WINDOW_SIZE + 1):\n",
    "        scaled_columns.append(f'sold_{previous}_weeks_ago')\n",
    "    \n",
    "    \n",
    "    scaled = scaler.fit_transform(df[scaled_columns])\n",
    "    scaled = pd.DataFrame(scaled, columns = scaled_columns)\n",
    "    \n",
    "    for col in scaled_columns:\n",
    "        df[col] = scaled[col].values\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "169ef87b-83c8-4fd3-bca3-beb75343a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_replace_knn(df : pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Function for computing new values from dataframe using kNN. \n",
    "        Params\n",
    "            df -> DataFrame of sales with NaN values as well.\n",
    "        Returns \n",
    "            Original Dataframe with replaced NaNs.\n",
    "        \"\"\"\n",
    "        \n",
    "        knn = KNNImputer()\n",
    "        knn_trans = knn.fit_transform(df)\n",
    "        return pd.DataFrame(knn_trans, columns = df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51e3411c-0414-44d4-b074-d0abf85ca8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAST_WINDOW_SIZE = 4\n",
    "\n",
    "def fill_zeroes_weekly(df: pd.DataFrame) -> pd.DataFrame: \n",
    "    \"\"\"\n",
    "    Function that takes DataFrame of sales and also fills weeks, where 0 products were sold. It also adds feature of last N weeks sold to DataFrame, as usefull features\n",
    "    Params\n",
    "        df -> DataFrame containing orders informations\n",
    "    Returns\n",
    "        DataFrame -> DataFrame containing weeks with 0 sales as well\n",
    "    \"\"\"\n",
    "    date_columns = 'doc_date'\n",
    "    desired_cols = ['year', 'week', 'month', 'quantity_sold', 'item_unit_price_with_vat',\n",
    "       'product_purchase_price', 'item_total_discount_with_vat', 'product_id',\n",
    "       'category_id', 'brand_id', 'segment_id', 'reviews_count',\n",
    "       'reviews_average_score_price', 'reviews_average_score_quality',\n",
    "       'reviews_average_score_properties', 'reviews_average_score_overall',\n",
    "       'reviews_average_score', 'product_status',\n",
    "        'ancestor_count',\n",
    "       'days_in_shop']\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        dates = pd.date_range(start=df.product_since.iloc[0], end=END_DATE)\n",
    "        weekly_sales = dates.isocalendar()[['year', 'week']]\n",
    "        weekly_sales['month'] = dates.month\n",
    "        weekly_sales = weekly_sales.drop_duplicates()\n",
    "    except:\n",
    "        # If there is no sale of given product\n",
    "        return pd.DataFrame(columns=desired_cols)\n",
    "\n",
    "    \n",
    "    sales, shop_days, item_price, item_discount, item_purchase_price = [], [], [], [], []\n",
    "    for wk in weekly_sales.iterrows():\n",
    "        if len(df[df.doc_week.__eq__(wk[1].week) & df.doc_month.__eq__(wk[1].month) & df.doc_year.__eq__(wk[1].year)].index) > 0:\n",
    "            sales.append(df[df.doc_week.__eq__(wk[1].week) & df.doc_month.__eq__(wk[1].month) & df.doc_year.__eq__(wk[1].year)].item_quantity.sum())\n",
    "            \n",
    "            item_price.append(df[df.doc_week.__eq__(wk[1].week) & df.doc_month.__eq__(wk[1].month) & df.doc_year.__eq__(wk[1].year)].item_unit_price_with_vat.mean())\n",
    "            item_purchase_price.append(df[df.doc_week.__eq__(wk[1].week) & df.doc_month.__eq__(wk[1].month) & df.doc_year.__eq__(wk[1].year)].product_purchase_price.mean())\n",
    "            item_discount.append(df[df.doc_week.__eq__(wk[1].week) & df.doc_month.__eq__(wk[1].month) & df.doc_year.__eq__(wk[1].year)].item_total_discount_with_vat.mean())\n",
    "            \n",
    "        else:\n",
    "            sales.append(0)\n",
    "            item_price.append(None)\n",
    "            item_purchase_price.append(None)\n",
    "            item_discount.append(None)\n",
    "        \n",
    "        shop_days.append((wk[0] - df.product_since.iloc[0]).days)\n",
    "    \n",
    "\n",
    "    weekly_sales['quantity_sold'] = sales\n",
    "    weekly_sales['item_unit_price_with_vat'] = item_price\n",
    "    weekly_sales['product_purchase_price'] = item_purchase_price\n",
    "    weekly_sales['item_total_discount_with_vat'] = item_discount\n",
    "    weekly_sales[['product_id', 'category_id', 'brand_id', 'segment_id']] = df.product_id.iloc[0], int(df.category_id.iloc[0]), df.brand_id.iloc[0], df.segment_id.iloc[0]\n",
    "    weekly_sales[['reviews_count', 'reviews_average_score_price', 'reviews_average_score_quality', 'reviews_average_score_properties', 'reviews_average_score_overall', 'reviews_average_score']] = \\\n",
    "    df.reviews_count.iloc[0], df.reviews_average_score_price.iloc[0], df.reviews_average_score_quality.iloc[0], df.reviews_average_score_properties.iloc[0], df.reviews_average_score_overall.iloc[0], df.reviews_average_score.iloc[0]\n",
    "    weekly_sales[['product_status', 'ancestor_count']] = df.product_status.iloc[0], df.ancestor_count.iloc[0]\n",
    "    weekly_sales['days_in_shop'] = shop_days\n",
    "       \n",
    " \n",
    "    try:\n",
    "        weekly_sales = missing_values_replace_knn(weekly_sales)\n",
    "    except:\n",
    "        return pd.DataFrame(columns=desired_cols)\n",
    "    \n",
    "    \n",
    "    for previous in range (1, PAST_WINDOW_SIZE + 1):\n",
    "        past_values = weekly_sales['quantity_sold']\n",
    "        weekly_sales[f'sold_{previous}_weeks_ago'] = past_values.shift(previous)\n",
    "\n",
    "    \n",
    "    weekly_sales\n",
    "    return weekly_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32d75541-0b7a-40b1-8214-a6cf293415a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_week(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    We want to have data of product sales based on weekly sales, for that we need to squish our data into weekly ones.\n",
    "    Params\n",
    "        df -> DataFrame containing data of sales\n",
    "    Return\n",
    "        DataFrame -> DataFrame where sales are groupped by week sales rather than daily\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.drop(labels=['doc_weekday', 'doc_day'], axis=1)\n",
    "\n",
    "    df = df.groupby(by=['product_id', 'doc_year', 'doc_month', 'doc_week'], as_index=False).agg({'item_quantity' : 'sum', \\\n",
    "                                                                                                   #  'item_type' : 'first', \\\n",
    "                                                                                                      'item_unit_price_with_vat' : 'mean', \\\n",
    "                                                                                                      'item_total_discount_with_vat' : 'mean', \\\n",
    "                                                                                                      'product_id' : 'first', \\\n",
    "                                                                                                      'category_id' : 'first', \\\n",
    "                                                                                                      'brand_id' : 'first', \\\n",
    "                                                                                                      'product_status' : 'first', \\\n",
    "                                                                                                      'reviews_count' : 'first', \\\n",
    "                                                                                                      'reviews_average_score_price' : 'first', \\\n",
    "                                                                                                      'reviews_average_score_quality' : 'first', \\\n",
    "                                                                                                      'reviews_average_score_properties' : 'first', \\\n",
    "                                                                                                      'reviews_average_score_overall' : 'first', \\\n",
    "                                                                                                      'reviews_average_score' : 'first', \\\n",
    "                                                                                                   #   'is_ended' : 'first', \\\n",
    "                                                                                                   #   'is_new' : 'first', \\\n",
    "                                                                                                   #   'is_boosted' : 'first', \\\n",
    "                                                                                                      'product_purchase_price' : 'mean', \\\n",
    "                                                                                                   #   'is_fifo' : 'first', \\\n",
    "                                                                                                   #   'category_status' : 'first', \\\n",
    "                                                                                                      'segment_id' : 'first', \\\n",
    "                                                                                                      'default_warranty_period' : 'first', \\\n",
    "                                                                                                      'days_in_shop' : 'first', \\\n",
    "                                                                                                      'ancestor_count' : 'first', \\\n",
    "                                                                                                   #   'descendant_count' : 'first', \\\n",
    "                                                                                                      'product_since' : 'first'                  \n",
    "                                                                                                      }) \n",
    "\n",
    "    \n",
    "    weekly_sales = pd.DataFrame()\n",
    "    for prod in df.product_id.unique():\n",
    "        weekly_sales = pd.concat([weekly_sales, fill_zeroes_weekly(df[df.product_id.__eq__(prod)])])\n",
    "\n",
    "    weekly_sales = weekly_sales.apply(pd.to_numeric)\n",
    "    \n",
    "    return weekly_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0622dc41-e735-401d-b76e-8e4a390d4726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cluster_data(df: pd.DataFrame, hierarchical_cluster : id, kmeans_cluster : id, normalized : bool =True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function that cleanse, prepares and returns data of weekly sales in given cluster.\n",
    "    Params\n",
    "        df - dataframe containing required data\n",
    "        hierarchical cluster - hierarchical cluster number, usually from <1, 3> interval\n",
    "        kmeans cluster - cluster decicded by kmeans clustering, usually from <1, 101> interval\n",
    "        normalized - default True. True returns normalized data in columns other than quantity_sold\n",
    "    Returns\n",
    "        pd.DataFrame - DataFrame containing weekly sales of each product in the cluster\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if required file is not generated already. If it is, return according csv file \n",
    "    check_existence = DATA_CLUSTERS_PATH+STANDARD_FILE_NAME+'_kmeans-'+str(kmeans_cluster)+'_hier-'+str(hierarchical_cluster)+'.csv'\n",
    "    if normalized:\n",
    "        check_existence = DATA_CLUSTERS_PATH+NORMALIZED_FILE_NAME+'_kmeans-'+str(kmeans_cluster)+'_hier-'+str(hierarchical_cluster)+'.csv'    \n",
    "        \n",
    "    if os.path.exists(check_existence):     \n",
    "        return pd.read_csv(check_existence, sep=',', low_memory=False)\n",
    "    \n",
    "    \n",
    "    selected_cluster = data[data.cluster_hierar.__eq__(hierarchical_cluster) & data.kmeans_cluster.__eq__(kmeans_cluster)]\n",
    "    \n",
    "    # Drop clusters as they are the same for each product of given cluster\n",
    "    selected_cluster = selected_cluster.drop(labels=['cluster_hierar', 'kmeans_cluster'], axis=1)\n",
    "    \n",
    "    \n",
    "    # Next we need to rework data to weekkly sales rather than daily \n",
    "    selected_cluster = group_week(selected_cluster)\n",
    "    \n",
    "    if normalized:\n",
    "        selected_cluster = encode_values(selected_cluster)\n",
    "        \n",
    "    return selected_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "558e10ed-e52e-4e20-8708-1c681d7c7a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_yearly_split(df : pd.DataFrame, normalized_year : bool =True, y_column : str='quantity_sold') -> tuple:\n",
    "    \"\"\"\n",
    "    Function splitting dataframe to train test and X, y parts based on year and quantity_sold parameters.\n",
    "    Params\n",
    "        df - pd.DataFrame containing desired columns for split\n",
    "        normalized_year - default True. Determine if the column 'year' is normalized or not. Needs this information for proper data split.\n",
    "        y_column - default quantity_sold. Column used as the y part of dataset\n",
    "    Returns\n",
    "        tuple - tuple containing prepared (normalized) and split data in order (X_train, y_train, X_test, y_test) - \n",
    "    \"\"\"\n",
    "    split_at_year = 2022\n",
    "    \n",
    "    if normalized_year: \n",
    "        split_at_year /= int(END_DATE[-4:])\n",
    "    \n",
    "    y_train = df[df.year < split_at_year][[y_column]]\n",
    "    y_test = df[df.year >= split_at_year][[y_column]]\n",
    "\n",
    "    X_train = df[df.year < split_at_year].drop(labels=[y_column], axis=1)\n",
    "    X_test = df[df.year >= split_at_year].drop(labels=[y_column], axis=1)\n",
    "    \n",
    "    return(X_train.fillna(0), y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8ad503d3-490e-417d-8c7b-02c17765116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_weekly_csvs(df : pd.DataFrame, normalized :bool = True):\n",
    "    \"\"\"\n",
    "    Function to generate csv files of weekly sales in advance, so those weekly sales don't have to be calculated every time the program is run, but are rather only loaded from .csv files.\n",
    "    This approach is trading memory (approximately 6GBs of files for one type (normalized VS standard)) - generally speaking, it is fully enough to generate all normalized files, there\n",
    "    is no need to generate standard ones. The tradeoff is saving computing power and speeding up the program for future runs.\n",
    "    \n",
    "    Approximate runtime on laptop: 8 hours.\n",
    "    Params \n",
    "        df - DataFrame containing orders information to sum weekly\n",
    "        normalized - default True. If to save normalized or full data\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    for kmns in range(101):\n",
    "        print(f'Working on kmeans cluster number {kmns}')\n",
    "        for hier in range(1, 4):\n",
    "            # path of the new file contaning normalized data prepared for the split\n",
    "            file_path = DATA_CLUSTERS_PATH+STANDARD_FILE_NAME+'_kmeans-'+str(kmns)+'_hier-'+str(hier)+'.csv'\n",
    "            if normalized:\n",
    "                file_path = DATA_CLUSTERS_PATH+NORMALIZED_FILE_NAME+'_kmeans-'+str(kmns)+'_hier-'+str(hier)+'.csv'\n",
    "        \n",
    "        \n",
    "            if not os.path.exists(file_path):\n",
    "                prepare_cluster_data(df, hier, kmns, normalized).to_csv(file_path, index=False)\n",
    "            else:\n",
    "                print(f'File already exists!: {file_path}')\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"{(end-start)/60} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e1c6cfcd-7c79-4678-865b-07debb26cfc6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on kmeans cluster number 0\n",
      "Working on kmeans cluster number 1\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-1_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-1_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-1_hier-3.csv\n",
      "Working on kmeans cluster number 2\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-2_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-2_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-2_hier-3.csv\n",
      "Working on kmeans cluster number 3\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-3_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-3_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-3_hier-3.csv\n",
      "Working on kmeans cluster number 4\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-4_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-4_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-4_hier-3.csv\n",
      "Working on kmeans cluster number 5\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-5_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-5_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-5_hier-3.csv\n",
      "Working on kmeans cluster number 6\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-6_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-6_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-6_hier-3.csv\n",
      "Working on kmeans cluster number 7\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-7_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-7_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-7_hier-3.csv\n",
      "Working on kmeans cluster number 8\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-8_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-8_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-8_hier-3.csv\n",
      "Working on kmeans cluster number 9\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-9_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-9_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-9_hier-3.csv\n",
      "Working on kmeans cluster number 10\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-10_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-10_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-10_hier-3.csv\n",
      "Working on kmeans cluster number 11\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-11_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-11_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-11_hier-3.csv\n",
      "Working on kmeans cluster number 12\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-12_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-12_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-12_hier-3.csv\n",
      "Working on kmeans cluster number 13\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-13_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-13_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-13_hier-3.csv\n",
      "Working on kmeans cluster number 14\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-14_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-14_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-14_hier-3.csv\n",
      "Working on kmeans cluster number 15\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-15_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-15_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-15_hier-3.csv\n",
      "Working on kmeans cluster number 16\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-16_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-16_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-16_hier-3.csv\n",
      "Working on kmeans cluster number 17\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-17_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-17_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-17_hier-3.csv\n",
      "Working on kmeans cluster number 18\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-18_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-18_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-18_hier-3.csv\n",
      "Working on kmeans cluster number 19\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-19_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-19_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-19_hier-3.csv\n",
      "Working on kmeans cluster number 20\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-20_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-20_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-20_hier-3.csv\n",
      "Working on kmeans cluster number 21\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-21_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-21_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-21_hier-3.csv\n",
      "Working on kmeans cluster number 22\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-22_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-22_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-22_hier-3.csv\n",
      "Working on kmeans cluster number 23\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-23_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-23_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-23_hier-3.csv\n",
      "Working on kmeans cluster number 24\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-24_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-24_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-24_hier-3.csv\n",
      "Working on kmeans cluster number 25\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-25_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-25_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-25_hier-3.csv\n",
      "Working on kmeans cluster number 26\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-26_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-26_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-26_hier-3.csv\n",
      "Working on kmeans cluster number 27\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-27_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-27_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-27_hier-3.csv\n",
      "Working on kmeans cluster number 28\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-28_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-28_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-28_hier-3.csv\n",
      "Working on kmeans cluster number 29\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-29_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-29_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-29_hier-3.csv\n",
      "Working on kmeans cluster number 30\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-30_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-30_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-30_hier-3.csv\n",
      "Working on kmeans cluster number 31\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-31_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-31_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-31_hier-3.csv\n",
      "Working on kmeans cluster number 32\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-32_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-32_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-32_hier-3.csv\n",
      "Working on kmeans cluster number 33\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-33_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-33_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-33_hier-3.csv\n",
      "Working on kmeans cluster number 34\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-34_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-34_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-34_hier-3.csv\n",
      "Working on kmeans cluster number 35\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-35_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-35_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-35_hier-3.csv\n",
      "Working on kmeans cluster number 36\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-36_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-36_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-36_hier-3.csv\n",
      "Working on kmeans cluster number 37\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-37_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-37_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-37_hier-3.csv\n",
      "Working on kmeans cluster number 38\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-38_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-38_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-38_hier-3.csv\n",
      "Working on kmeans cluster number 39\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-39_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-39_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-39_hier-3.csv\n",
      "Working on kmeans cluster number 40\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-40_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-40_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-40_hier-3.csv\n",
      "Working on kmeans cluster number 41\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-41_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-41_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-41_hier-3.csv\n",
      "Working on kmeans cluster number 42\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-42_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-42_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-42_hier-3.csv\n",
      "Working on kmeans cluster number 43\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-43_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-43_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-43_hier-3.csv\n",
      "Working on kmeans cluster number 44\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-44_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-44_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-44_hier-3.csv\n",
      "Working on kmeans cluster number 45\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-45_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-45_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-45_hier-3.csv\n",
      "Working on kmeans cluster number 46\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-46_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-46_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-46_hier-3.csv\n",
      "Working on kmeans cluster number 47\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-47_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-47_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-47_hier-3.csv\n",
      "Working on kmeans cluster number 48\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-48_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-48_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-48_hier-3.csv\n",
      "Working on kmeans cluster number 49\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-49_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-49_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-49_hier-3.csv\n",
      "Working on kmeans cluster number 50\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-50_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-50_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-50_hier-3.csv\n",
      "Working on kmeans cluster number 51\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-51_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-51_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-51_hier-3.csv\n",
      "Working on kmeans cluster number 52\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-52_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-52_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-52_hier-3.csv\n",
      "Working on kmeans cluster number 53\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-53_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-53_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-53_hier-3.csv\n",
      "Working on kmeans cluster number 54\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-54_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-54_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-54_hier-3.csv\n",
      "Working on kmeans cluster number 55\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-55_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-55_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-55_hier-3.csv\n",
      "Working on kmeans cluster number 56\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-56_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-56_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-56_hier-3.csv\n",
      "Working on kmeans cluster number 57\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-57_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-57_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-57_hier-3.csv\n",
      "Working on kmeans cluster number 58\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-58_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-58_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-58_hier-3.csv\n",
      "Working on kmeans cluster number 59\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-59_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-59_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-59_hier-3.csv\n",
      "Working on kmeans cluster number 60\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-60_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-60_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-60_hier-3.csv\n",
      "Working on kmeans cluster number 61\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-61_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-61_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-61_hier-3.csv\n",
      "Working on kmeans cluster number 62\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-62_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-62_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-62_hier-3.csv\n",
      "Working on kmeans cluster number 63\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-63_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-63_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-63_hier-3.csv\n",
      "Working on kmeans cluster number 64\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-64_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-64_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-64_hier-3.csv\n",
      "Working on kmeans cluster number 65\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-65_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-65_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-65_hier-3.csv\n",
      "Working on kmeans cluster number 66\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-66_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-66_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-66_hier-3.csv\n",
      "Working on kmeans cluster number 67\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-67_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-67_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-67_hier-3.csv\n",
      "Working on kmeans cluster number 68\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-68_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-68_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-68_hier-3.csv\n",
      "Working on kmeans cluster number 69\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-69_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-69_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-69_hier-3.csv\n",
      "Working on kmeans cluster number 70\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-70_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-70_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-70_hier-3.csv\n",
      "Working on kmeans cluster number 71\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-71_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-71_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-71_hier-3.csv\n",
      "Working on kmeans cluster number 72\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-72_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-72_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-72_hier-3.csv\n",
      "Working on kmeans cluster number 73\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-73_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-73_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-73_hier-3.csv\n",
      "Working on kmeans cluster number 74\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-74_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-74_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-74_hier-3.csv\n",
      "Working on kmeans cluster number 75\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-75_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-75_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-75_hier-3.csv\n",
      "Working on kmeans cluster number 76\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-76_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-76_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-76_hier-3.csv\n",
      "Working on kmeans cluster number 77\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-77_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-77_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-77_hier-3.csv\n",
      "Working on kmeans cluster number 78\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-78_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-78_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-78_hier-3.csv\n",
      "Working on kmeans cluster number 79\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-79_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-79_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-79_hier-3.csv\n",
      "Working on kmeans cluster number 80\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-80_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-80_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-80_hier-3.csv\n",
      "Working on kmeans cluster number 81\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-81_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-81_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-81_hier-3.csv\n",
      "Working on kmeans cluster number 82\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-82_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-82_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-82_hier-3.csv\n",
      "Working on kmeans cluster number 83\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-83_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-83_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-83_hier-3.csv\n",
      "Working on kmeans cluster number 84\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-84_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-84_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-84_hier-3.csv\n",
      "Working on kmeans cluster number 85\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-85_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-85_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-85_hier-3.csv\n",
      "Working on kmeans cluster number 86\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-86_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-86_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-86_hier-3.csv\n",
      "Working on kmeans cluster number 87\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-87_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-87_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-87_hier-3.csv\n",
      "Working on kmeans cluster number 88\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-88_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-88_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-88_hier-3.csv\n",
      "Working on kmeans cluster number 89\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-89_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-89_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-89_hier-3.csv\n",
      "Working on kmeans cluster number 90\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-90_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-90_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-90_hier-3.csv\n",
      "Working on kmeans cluster number 91\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-91_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-91_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-91_hier-3.csv\n",
      "Working on kmeans cluster number 92\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-92_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-92_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-92_hier-3.csv\n",
      "Working on kmeans cluster number 93\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-93_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-93_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-93_hier-3.csv\n",
      "Working on kmeans cluster number 94\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-94_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-94_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-94_hier-3.csv\n",
      "Working on kmeans cluster number 95\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-95_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-95_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-95_hier-3.csv\n",
      "Working on kmeans cluster number 96\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-96_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-96_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-96_hier-3.csv\n",
      "Working on kmeans cluster number 97\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-97_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-97_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-97_hier-3.csv\n",
      "Working on kmeans cluster number 98\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-98_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-98_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-98_hier-3.csv\n",
      "Working on kmeans cluster number 99\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-99_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-99_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-99_hier-3.csv\n",
      "Working on kmeans cluster number 100\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-100_hier-1.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-100_hier-2.csv\n",
      "File already exists!: data/clusters/normalized_prepared_kmeans-100_hier-3.csv\n",
      "11.647693367799123 minutes.\n"
     ]
    }
   ],
   "source": [
    "prepare_weekly_csvs(data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37506d17-ef75-430e-a586-ec2454fc4f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_xgb(df : pd.DataFrame, TEST_SIZE : int = 10, model : XGBRegressor = None) -> dict:\n",
    "    \"\"\"\n",
    "    Let's check the importance of remaining features based on TEST_SIZE random cluster predictions.\n",
    "    Params\n",
    "        df - DataFrame containing information about product sales.\n",
    "        TEST_SIZE - Default 10, how many random models should be tested to find out the values importance.\n",
    "        model - XGB Regressor model to test on. If None, then there is predefined model used in the function.\n",
    "    Returns\n",
    "        dictionary - containing all feature names and their importances - averaged from the entire TEST_SIZE of trees.\n",
    "    \"\"\"\n",
    "    tested = [[]]\n",
    "    xgb_feature_importance = []\n",
    "    \n",
    "    for check in range (TEST_SIZE):\n",
    "        \n",
    "        test_cluster = []\n",
    "        while test_cluster in tested:\n",
    "            test_cluster = [random.randint(1, 100), random.randint(1, 3)] \n",
    "        \n",
    "        tested.append(test_cluster)\n",
    "\n",
    "        X_train, y_train, X_test, y_test = test_train_yearly_split(prepare_cluster_data(df, test_cluster[1], test_cluster[0]))\n",
    "\n",
    "        if model is None:\n",
    "            model = XGBRegressor(learning_rate = 0.05, n_estimators=250,\n",
    "                           max_depth=4, min_child_weight=1,\n",
    "                           gamma=0, subsample=0.8, reg_lambda=10,       \n",
    "                           early_metric='auc',\n",
    "                           eval_set=[(X_test, y_test)],\n",
    "                           colsample_bytree=0.8, objective= \"reg:squarederror\",  \n",
    "                           nthread=-1,scale_pos_weight=1, seed=27,\n",
    "                          importance_type='total_gain')\n",
    "        \n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        if check == 0:\n",
    "            xgb_feature_importance = xgb.feature_importances_\n",
    "        else:\n",
    "            xgb_feature_importance += xgb.feature_importances_\n",
    "            \n",
    "        print(f'Done {check+1} out of {TEST_SIZE} models. ({round((1+check)*100/TEST_SIZE, 2)}%)')\n",
    "    \n",
    "    names, values = (xgb.get_booster().feature_names, xgb_feature_importance / TEST_SIZE)\n",
    "    return[{name: value} for (name, value) in zip(names, values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9fe144b-41d8-4509-9eff-d84c0c5dfba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(X_train_data : pd.DataFrame, X_test_data : pd.DataFrame, y_train_data : pd.DataFrame, \\\n",
    "                          y_test_data : pd.DataFrame , model, param_grid : dict, cv : int =10, scoring_fit  : str ='neg_mean_squared_error', n_iterations : int = 10):\n",
    "    \"\"\"\n",
    "    Function using RandomizedSearch to tune hyperparameters to find very good fitting parameters in a reasonable time. Can be swapped for GridSearchCV, but that is way slower.\n",
    "    Params\n",
    "        X_train_data, X_test_data, y_train_data, y_test_data - DataFrames containing splitted train and test data.\n",
    "        model - MachineLearning model to be tested.\n",
    "        param_grid - Grid of parameters with their runing values..\n",
    "        scoring_fit - string according to scroring options of RandomizedSearachCV. Default is neg_MAE.\n",
    "        n_iterations - amount of models to test in the RandomizedSearch.\n",
    "    Returns\n",
    "        fitted_model - best model found\n",
    "        pred - prediction made on the test dataset\n",
    "    \"\"\"\n",
    "    rs = RandomizedSearchCV(\n",
    "            estimator=model,\n",
    "            param_distributions=param_grid, \n",
    "            cv=cv, \n",
    "            n_jobs=1, \n",
    "            n_iter=n_iterations,\n",
    "            scoring=scoring_fit,\n",
    "            verbose=5\n",
    "        )\n",
    "    fitted_model = rs.fit(X_train_data, y_train_data)\n",
    "\n",
    "    return fitted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f671f21b-62cd-424a-9ada-ca2ddc838f18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(real_numbers : list, predicted_numbers : list) -> dict:\n",
    "    \"\"\"\n",
    "    Method which uses various metrics to evaluate predictions compared to the real numbers.\n",
    "    More info about following techniques of evaluation available in the documentation: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "    Params\n",
    "        real_numbers - list/np.array/Series containing true values to compare against \n",
    "        predicted_numbers - list/np.array/Series containing predicted values which we want to evaluate\n",
    "    Returns \n",
    "        dict - dictinoary containing scores of different evaluation methods\n",
    "    \"\"\"\n",
    "    round_size = 4\n",
    "    \n",
    "    ### 3 main metrics used to evaluate regressions. Lower values are better. They all compute average error size, in different ways\n",
    "    medae = round(median_absolute_error(real_numbers, predicted_numbers), round_size)\n",
    "    mae = round(mean_absolute_error(real_numbers, predicted_numbers), round_size)\n",
    "    mse = round(mean_squared_error(real_numbers, predicted_numbers), round_size)\n",
    "    \n",
    "    # The F1 score can be interpreted as a harmonic mean of the precision and recall. The best value is 1.0.\n",
    "    f1 = round(f1_score(real_numbers, predicted_numbers.round(), average='weighted', zero_division=1), round_size)\n",
    "    \n",
    "    ### 3 more metrics used to evaluate models. They all have the best values at 1\n",
    "    accuracy = round(accuracy_score(real_numbers, predicted_numbers.round()), round_size)\n",
    "    recall = round(recall_score(real_numbers, predicted_numbers.round(), average='weighted', zero_division=1), round_size)\n",
    "    precision = round(precision_score(real_numbers, predicted_numbers.round(), average='weighted', zero_division=1), round_size)\n",
    "    \n",
    "    # Regression metrics are important for regressions, so we want to have their average as score of options\n",
    "    regress = round((mae + medae + mse) / 3, round_size)\n",
    "    \n",
    "    # Calculate total score of the model - find the average distance of metrics from the ideal score\n",
    "    total = medae + mae + mse + (1 - f1) + (1 - accuracy) + (1 - recall) + (1 - precision)\n",
    "    total /= 7\n",
    "    total = round(total, round_size)\n",
    "    \n",
    "    return {'medae' : medae, 'mae' : mae, 'mse' : mse , 'f1' : f1, 'accuracy' : accuracy, 'recall' : recall, 'precision' : precision, 'regress' : regress, 'total' : total}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "aef31bce-21dd-4be2-84d8-392274594e34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_evaluate_model(df : pd.DataFrame, model, hier : int, kmeans : int, tune_hyperparameters : bool = False, parameters_to_tune : dict = {}) -> dict:\n",
    "    \"\"\"\n",
    "    Function that prepares desired cluster data, runs inserted model on them and evaluates the outcome.\n",
    "    Params\n",
    "        df - DataFrame containing sales information\n",
    "        hier - hierarchical cluster ID\n",
    "        kmeans - kmeans cluster ID\n",
    "        model - Regression model to test\n",
    "        tune_hyperparameters - default False. If True, use RandomSearchCV on given cluster to find hyperparameters. If True, requires parameters_train as well.\n",
    "        parameters_to_tune - default {}. Dictionary of parameters to tune on given model.\n",
    "        \n",
    "    Returns\n",
    "        dict - dictionary containing evaluation values of model over data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select data and make split\n",
    "    selected_cluster = prepare_cluster_data(df, hier, kmeans, normalized=True)\n",
    "    X_train, y_train, X_test, y_test = test_train_yearly_split(selected_cluster)\n",
    "    y_train = y_train.quantity_sold.values\n",
    "    y_test = y_test.quantity_sold.values\n",
    "    \n",
    "    if tune_hyperparameters:\n",
    "        # Tune hyperparameters with the usage of RandomSearch\n",
    "        model, _ = hyperparameter_tuning(X_train, X_test, y_train, y_test, xgb_r, parameters_to_tune, cv=2, n_iterations=50)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    predicted_train = model.predict(X_train)\n",
    "    predicted_test = model.predict(X_test)\n",
    "    \n",
    "    train_evaluated = evaluate_model(y_train, predicted_train)\n",
    "    test_evaluated = evaluate_model(y_test, predicted_test)\n",
    "    \n",
    "    return {f'kmeans-{kmeans}_hierarchical-{hier}' : {'train' : train_evaluated, 'test' : test_evaluated}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccba290-cf29-4405-9de6-de06a6825709",
   "metadata": {},
   "source": [
    "# XGBoost regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07bdfb19-cb50-4379-90a6-298d5484e588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training parameters for XGBoost\n",
    "parameters_XGB = { \n",
    "    'booster' : ['gbtree', 'gblinear'], \n",
    "    'eta' : [0.01, 0.05, 0.1, 0.2, 0.3, 0.6, 1],   # learning rate\n",
    "    'n_estimators': [250, 400, 700, 1000],\n",
    "    'colsample_bytree': [0.4, 0.6, 0.8, 1.0],\n",
    "    'max_depth': [3, 5, 7, 10, 15],\n",
    "    'reg_alpha': [0, 0.5 , 1, 1.1, 1.3],\n",
    "    'reg_lambda': [0.5, 1, 1.1, 1.3],\n",
    "    'subsample': [0.4, 0.6, 0.7, 0.8],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "    'objective': ['reg:squarederror', 'reg:squaredlogerror'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79da20c6-6b59-4ba3-9b6f-538b2006dd90",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.8, eta=0.6, gamma=5, max_depth=10, min_child_weight=10, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=0, reg_lambda=1, subsample=0.4;, score=-0.021 total time=   2.0s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.8, eta=0.6, gamma=5, max_depth=10, min_child_weight=10, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=0, reg_lambda=1, subsample=0.4;, score=-0.020 total time=   1.6s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.6, eta=0.1, gamma=2, max_depth=10, min_child_weight=10, n_estimators=250, objective=reg:squaredlogerror, reg_alpha=0, reg_lambda=1, subsample=0.4;, score=-0.021 total time=   4.8s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.6, eta=0.1, gamma=2, max_depth=10, min_child_weight=10, n_estimators=250, objective=reg:squaredlogerror, reg_alpha=0, reg_lambda=1, subsample=0.4;, score=-0.020 total time=   4.0s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.8, eta=0.6, gamma=0.5, max_depth=3, min_child_weight=10, n_estimators=400, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1.1, subsample=0.8;, score=-0.021 total time=   1.3s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.8, eta=0.6, gamma=0.5, max_depth=3, min_child_weight=10, n_estimators=400, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1.1, subsample=0.8;, score=-0.020 total time=   1.6s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.8, eta=0.2, gamma=1.5, max_depth=15, min_child_weight=10, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=0, reg_lambda=1.1, subsample=0.4;, score=-0.021 total time=  10.0s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.8, eta=0.2, gamma=1.5, max_depth=15, min_child_weight=10, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=0, reg_lambda=1.1, subsample=0.4;, score=-0.020 total time=  12.2s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.8, eta=0.2, gamma=1.5, max_depth=7, min_child_weight=10, n_estimators=400, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=0.5, subsample=0.8;, score=-0.032 total time=   6.0s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.8, eta=0.2, gamma=1.5, max_depth=7, min_child_weight=10, n_estimators=400, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=0.5, subsample=0.8;, score=-0.021 total time=   7.9s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.4, eta=0.1, gamma=5, max_depth=15, min_child_weight=10, n_estimators=700, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1.1, subsample=0.7;, score=-0.022 total time=  13.6s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.4, eta=0.1, gamma=5, max_depth=15, min_child_weight=10, n_estimators=700, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1.1, subsample=0.7;, score=-0.020 total time=  13.0s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.8, eta=0.3, gamma=0.5, max_depth=10, min_child_weight=10, n_estimators=250, objective=reg:squaredlogerror, reg_alpha=0.5, reg_lambda=0.5, subsample=0.7;, score=-0.021 total time=   0.9s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.8, eta=0.3, gamma=0.5, max_depth=10, min_child_weight=10, n_estimators=250, objective=reg:squaredlogerror, reg_alpha=0.5, reg_lambda=0.5, subsample=0.7;, score=-0.020 total time=   0.9s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.8, eta=0.05, gamma=5, max_depth=15, min_child_weight=1, n_estimators=1000, objective=reg:squaredlogerror, reg_alpha=1.1, reg_lambda=1, subsample=0.6;, score=-0.021 total time=   3.6s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.8, eta=0.05, gamma=5, max_depth=15, min_child_weight=1, n_estimators=1000, objective=reg:squaredlogerror, reg_alpha=1.1, reg_lambda=1, subsample=0.6;, score=-0.020 total time=   3.8s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.4, eta=1, gamma=1.5, max_depth=5, min_child_weight=5, n_estimators=1000, objective=reg:squarederror, reg_alpha=0, reg_lambda=1.1, subsample=0.8;, score=-0.069 total time=   8.6s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.4, eta=1, gamma=1.5, max_depth=5, min_child_weight=5, n_estimators=1000, objective=reg:squarederror, reg_alpha=0, reg_lambda=1.1, subsample=0.8;, score=-0.026 total time=   8.4s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.6, eta=0.3, gamma=5, max_depth=3, min_child_weight=10, n_estimators=1000, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=0.5, subsample=0.6;, score=-0.021 total time=   7.8s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.6, eta=0.3, gamma=5, max_depth=3, min_child_weight=10, n_estimators=1000, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=0.5, subsample=0.6;, score=-0.020 total time=   8.1s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.4, eta=1, gamma=1.5, max_depth=7, min_child_weight=10, n_estimators=1000, objective=reg:squarederror, reg_alpha=0, reg_lambda=1, subsample=0.4;, score=-0.066 total time=  10.9s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.4, eta=1, gamma=1.5, max_depth=7, min_child_weight=10, n_estimators=1000, objective=reg:squarederror, reg_alpha=0, reg_lambda=1, subsample=0.4;, score=-0.037 total time=  10.9s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.4, eta=0.05, gamma=1, max_depth=5, min_child_weight=10, n_estimators=1000, objective=reg:squarederror, reg_alpha=1.3, reg_lambda=1.3, subsample=0.8;, score=-0.022 total time=   7.9s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.4, eta=0.05, gamma=1, max_depth=5, min_child_weight=10, n_estimators=1000, objective=reg:squarederror, reg_alpha=1.3, reg_lambda=1.3, subsample=0.8;, score=-0.020 total time=   8.0s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.6, eta=0.05, gamma=1.5, max_depth=7, min_child_weight=1, n_estimators=700, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1, subsample=0.6;, score=-0.021 total time=   2.5s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.6, eta=0.05, gamma=1.5, max_depth=7, min_child_weight=1, n_estimators=700, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1, subsample=0.6;, score=-0.020 total time=   2.5s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.4, eta=0.2, gamma=1, max_depth=15, min_child_weight=1, n_estimators=700, objective=reg:squarederror, reg_alpha=1, reg_lambda=0.5, subsample=0.6;, score=-0.035 total time=  16.1s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.4, eta=0.2, gamma=1, max_depth=15, min_child_weight=1, n_estimators=700, objective=reg:squarederror, reg_alpha=1, reg_lambda=0.5, subsample=0.6;, score=-0.025 total time=  16.4s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.8, eta=0.01, gamma=0.5, max_depth=7, min_child_weight=5, n_estimators=700, objective=reg:squarederror, reg_alpha=1.1, reg_lambda=1, subsample=0.8;, score=-0.028 total time=   8.6s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.8, eta=0.01, gamma=0.5, max_depth=7, min_child_weight=5, n_estimators=700, objective=reg:squarederror, reg_alpha=1.1, reg_lambda=1, subsample=0.8;, score=-0.020 total time=   8.7s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=1.0, eta=0.6, gamma=1, max_depth=15, min_child_weight=10, n_estimators=400, objective=reg:squarederror, reg_alpha=1.3, reg_lambda=0.5, subsample=0.6;, score=-0.021 total time=   1.6s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=1.0, eta=0.6, gamma=1, max_depth=15, min_child_weight=10, n_estimators=400, objective=reg:squarederror, reg_alpha=1.3, reg_lambda=0.5, subsample=0.6;, score=-0.020 total time=   1.5s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.4, eta=0.05, gamma=1, max_depth=3, min_child_weight=5, n_estimators=250, objective=reg:squaredlogerror, reg_alpha=1, reg_lambda=1, subsample=0.8;, score=-0.021 total time=   1.9s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.4, eta=0.05, gamma=1, max_depth=3, min_child_weight=5, n_estimators=250, objective=reg:squaredlogerror, reg_alpha=1, reg_lambda=1, subsample=0.8;, score=-0.020 total time=   1.8s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.8, eta=0.05, gamma=0.5, max_depth=15, min_child_weight=5, n_estimators=400, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1.1, subsample=0.6;, score=-0.021 total time=   1.4s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.8, eta=0.05, gamma=0.5, max_depth=15, min_child_weight=5, n_estimators=400, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1.1, subsample=0.6;, score=-0.020 total time=   1.7s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=1.0, eta=1, gamma=2, max_depth=15, min_child_weight=1, n_estimators=250, objective=reg:squaredlogerror, reg_alpha=0, reg_lambda=1.3, subsample=0.7;, score=-0.022 total time=   8.9s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=1.0, eta=1, gamma=2, max_depth=15, min_child_weight=1, n_estimators=250, objective=reg:squaredlogerror, reg_alpha=0, reg_lambda=1.3, subsample=0.7;, score=-0.020 total time=   9.4s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.8, eta=1, gamma=1.5, max_depth=3, min_child_weight=10, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=1.3, reg_lambda=0.5, subsample=0.6;, score=-0.021 total time=   1.6s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.8, eta=1, gamma=1.5, max_depth=3, min_child_weight=10, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=1.3, reg_lambda=0.5, subsample=0.6;, score=-0.020 total time=   1.6s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.4, eta=1, gamma=1.5, max_depth=3, min_child_weight=10, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=0, reg_lambda=1.3, subsample=0.8;, score=-0.021 total time=   2.8s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.4, eta=1, gamma=1.5, max_depth=3, min_child_weight=10, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=0, reg_lambda=1.3, subsample=0.8;, score=-0.020 total time=   2.7s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.6, eta=0.2, gamma=5, max_depth=3, min_child_weight=1, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=0.5, reg_lambda=1, subsample=0.8;, score=-0.021 total time=   1.6s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.6, eta=0.2, gamma=5, max_depth=3, min_child_weight=1, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=0.5, reg_lambda=1, subsample=0.8;, score=-0.020 total time=   1.7s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.8, eta=0.3, gamma=1.5, max_depth=10, min_child_weight=1, n_estimators=1000, objective=reg:squarederror, reg_alpha=0, reg_lambda=1.3, subsample=0.8;, score=-0.021 total time=   3.9s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.8, eta=0.3, gamma=1.5, max_depth=10, min_child_weight=1, n_estimators=1000, objective=reg:squarederror, reg_alpha=0, reg_lambda=1.3, subsample=0.8;, score=-0.020 total time=   3.9s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.6, eta=0.1, gamma=0.5, max_depth=5, min_child_weight=1, n_estimators=1000, objective=reg:squaredlogerror, reg_alpha=1.1, reg_lambda=1.1, subsample=0.4;, score=-0.021 total time=   9.9s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.6, eta=0.1, gamma=0.5, max_depth=5, min_child_weight=1, n_estimators=1000, objective=reg:squaredlogerror, reg_alpha=1.1, reg_lambda=1.1, subsample=0.4;, score=-0.020 total time=   9.9s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.6, eta=0.1, gamma=5, max_depth=5, min_child_weight=10, n_estimators=250, objective=reg:squaredlogerror, reg_alpha=1.1, reg_lambda=1, subsample=0.6;, score=-0.021 total time=   2.7s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.6, eta=0.1, gamma=5, max_depth=5, min_child_weight=10, n_estimators=250, objective=reg:squaredlogerror, reg_alpha=1.1, reg_lambda=1, subsample=0.6;, score=-0.020 total time=   2.7s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.8, eta=0.6, gamma=5, max_depth=5, min_child_weight=10, n_estimators=700, objective=reg:squarederror, reg_alpha=1, reg_lambda=1, subsample=0.4;, score=-0.022 total time=   7.7s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.8, eta=0.6, gamma=5, max_depth=5, min_child_weight=10, n_estimators=700, objective=reg:squarederror, reg_alpha=1, reg_lambda=1, subsample=0.4;, score=-0.020 total time=   7.7s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.8, eta=0.01, gamma=0.5, max_depth=5, min_child_weight=10, n_estimators=250, objective=reg:squarederror, reg_alpha=1.1, reg_lambda=1.3, subsample=0.6;, score=-0.022 total time=   0.8s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.8, eta=0.01, gamma=0.5, max_depth=5, min_child_weight=10, n_estimators=250, objective=reg:squarederror, reg_alpha=1.1, reg_lambda=1.3, subsample=0.6;, score=-0.021 total time=   0.8s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.6, eta=0.2, gamma=0.5, max_depth=7, min_child_weight=10, n_estimators=400, objective=reg:squarederror, reg_alpha=1, reg_lambda=1, subsample=0.4;, score=-0.021 total time=   1.3s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.6, eta=0.2, gamma=0.5, max_depth=7, min_child_weight=10, n_estimators=400, objective=reg:squarederror, reg_alpha=1, reg_lambda=1, subsample=0.4;, score=-0.020 total time=   1.7s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.6, eta=0.1, gamma=1, max_depth=7, min_child_weight=5, n_estimators=1000, objective=reg:squarederror, reg_alpha=1.3, reg_lambda=1, subsample=0.6;, score=-0.021 total time=   3.4s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.6, eta=0.1, gamma=1, max_depth=7, min_child_weight=5, n_estimators=1000, objective=reg:squarederror, reg_alpha=1.3, reg_lambda=1, subsample=0.6;, score=-0.020 total time=   3.4s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.4, eta=0.3, gamma=2, max_depth=10, min_child_weight=5, n_estimators=400, objective=reg:squarederror, reg_alpha=0, reg_lambda=1.1, subsample=0.8;, score=-0.042 total time=   6.8s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.4, eta=0.3, gamma=2, max_depth=10, min_child_weight=5, n_estimators=400, objective=reg:squarederror, reg_alpha=0, reg_lambda=1.1, subsample=0.8;, score=-0.021 total time=   6.8s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=1.0, eta=1, gamma=2, max_depth=3, min_child_weight=1, n_estimators=700, objective=reg:squarederror, reg_alpha=1, reg_lambda=1.1, subsample=0.8;, score=-0.021 total time=   2.5s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=1.0, eta=1, gamma=2, max_depth=3, min_child_weight=1, n_estimators=700, objective=reg:squarederror, reg_alpha=1, reg_lambda=1.1, subsample=0.8;, score=-0.020 total time=   2.4s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.6, eta=0.1, gamma=0.5, max_depth=5, min_child_weight=10, n_estimators=700, objective=reg:squaredlogerror, reg_alpha=1.3, reg_lambda=0.5, subsample=0.8;, score=-0.021 total time=   6.9s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.6, eta=0.1, gamma=0.5, max_depth=5, min_child_weight=10, n_estimators=700, objective=reg:squaredlogerror, reg_alpha=1.3, reg_lambda=0.5, subsample=0.8;, score=-0.020 total time=   6.9s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.6, eta=0.1, gamma=1.5, max_depth=5, min_child_weight=10, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=-0.021 total time=   4.3s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.6, eta=0.1, gamma=1.5, max_depth=5, min_child_weight=10, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=-0.020 total time=   4.3s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.8, eta=0.3, gamma=1.5, max_depth=15, min_child_weight=1, n_estimators=250, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=0.5, subsample=0.7;, score=-0.021 total time=   0.9s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.8, eta=0.3, gamma=1.5, max_depth=15, min_child_weight=1, n_estimators=250, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=0.5, subsample=0.7;, score=-0.020 total time=   0.9s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=1.0, eta=0.2, gamma=0.5, max_depth=10, min_child_weight=10, n_estimators=700, objective=reg:squarederror, reg_alpha=1, reg_lambda=1.3, subsample=0.7;, score=-0.021 total time=   2.9s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=1.0, eta=0.2, gamma=0.5, max_depth=10, min_child_weight=10, n_estimators=700, objective=reg:squarederror, reg_alpha=1, reg_lambda=1.3, subsample=0.7;, score=-0.020 total time=   2.5s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.6, eta=0.05, gamma=5, max_depth=7, min_child_weight=10, n_estimators=400, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1.3, subsample=0.6;, score=-0.021 total time=   5.3s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.6, eta=0.05, gamma=5, max_depth=7, min_child_weight=10, n_estimators=400, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1.3, subsample=0.6;, score=-0.020 total time=   5.1s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=1.0, eta=0.01, gamma=0.5, max_depth=7, min_child_weight=5, n_estimators=700, objective=reg:squarederror, reg_alpha=1, reg_lambda=1.1, subsample=0.4;, score=-0.021 total time=   2.5s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=1.0, eta=0.01, gamma=0.5, max_depth=7, min_child_weight=5, n_estimators=700, objective=reg:squarederror, reg_alpha=1, reg_lambda=1.1, subsample=0.4;, score=-0.020 total time=   2.5s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.8, eta=0.2, gamma=2, max_depth=5, min_child_weight=5, n_estimators=700, objective=reg:squaredlogerror, reg_alpha=0.5, reg_lambda=1, subsample=0.8;, score=-0.021 total time=   2.6s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.8, eta=0.2, gamma=2, max_depth=5, min_child_weight=5, n_estimators=700, objective=reg:squaredlogerror, reg_alpha=0.5, reg_lambda=1, subsample=0.8;, score=-0.020 total time=   2.6s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=1.0, eta=0.2, gamma=2, max_depth=3, min_child_weight=1, n_estimators=250, objective=reg:squaredlogerror, reg_alpha=1, reg_lambda=1, subsample=0.7;, score=-0.021 total time=   0.9s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=1.0, eta=0.2, gamma=2, max_depth=3, min_child_weight=1, n_estimators=250, objective=reg:squaredlogerror, reg_alpha=1, reg_lambda=1, subsample=0.7;, score=-0.020 total time=   0.9s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.4, eta=0.05, gamma=1.5, max_depth=15, min_child_weight=10, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=0.5, reg_lambda=1.1, subsample=0.6;, score=-0.021 total time=   1.6s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.4, eta=0.05, gamma=1.5, max_depth=15, min_child_weight=10, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=0.5, reg_lambda=1.1, subsample=0.6;, score=-0.020 total time=   1.6s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.8, eta=1, gamma=5, max_depth=3, min_child_weight=10, n_estimators=400, objective=reg:squarederror, reg_alpha=0, reg_lambda=1.1, subsample=0.4;, score=-0.021 total time=   3.4s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.8, eta=1, gamma=5, max_depth=3, min_child_weight=10, n_estimators=400, objective=reg:squarederror, reg_alpha=0, reg_lambda=1.1, subsample=0.4;, score=-0.020 total time=   3.5s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.4, eta=0.2, gamma=0.5, max_depth=10, min_child_weight=10, n_estimators=700, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1, subsample=0.7;, score=-0.021 total time=   2.5s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.4, eta=0.2, gamma=0.5, max_depth=10, min_child_weight=10, n_estimators=700, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1, subsample=0.7;, score=-0.020 total time=   2.6s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.4, eta=1, gamma=2, max_depth=10, min_child_weight=1, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=0, reg_lambda=0.5, subsample=0.8;, score=-0.021 total time=   1.9s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.4, eta=1, gamma=2, max_depth=10, min_child_weight=1, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=0, reg_lambda=0.5, subsample=0.8;, score=-0.020 total time=   1.8s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.4, eta=1, gamma=2, max_depth=10, min_child_weight=5, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=1, reg_lambda=1.3, subsample=0.4;, score=-0.021 total time=   5.3s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.4, eta=1, gamma=2, max_depth=10, min_child_weight=5, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=1, reg_lambda=1.3, subsample=0.4;, score=-0.020 total time=   5.2s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.8, eta=0.3, gamma=5, max_depth=15, min_child_weight=1, n_estimators=250, objective=reg:squarederror, reg_alpha=1.1, reg_lambda=1.1, subsample=0.6;, score=-0.026 total time=   7.1s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.8, eta=0.3, gamma=5, max_depth=15, min_child_weight=1, n_estimators=250, objective=reg:squarederror, reg_alpha=1.1, reg_lambda=1.1, subsample=0.6;, score=-0.020 total time=   7.9s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=1.0, eta=1, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=250, objective=reg:squaredlogerror, reg_alpha=1.1, reg_lambda=1.3, subsample=0.4;, score=-0.021 total time=   0.9s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=1.0, eta=1, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=250, objective=reg:squaredlogerror, reg_alpha=1.1, reg_lambda=1.3, subsample=0.4;, score=-0.020 total time=   0.9s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.4, eta=0.1, gamma=2, max_depth=7, min_child_weight=5, n_estimators=1000, objective=reg:squarederror, reg_alpha=1, reg_lambda=0.5, subsample=0.6;, score=-0.031 total time=  10.9s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.4, eta=0.1, gamma=2, max_depth=7, min_child_weight=5, n_estimators=1000, objective=reg:squarederror, reg_alpha=1, reg_lambda=0.5, subsample=0.6;, score=-0.020 total time=  10.9s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.4, eta=0.05, gamma=1, max_depth=15, min_child_weight=5, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=0.5, reg_lambda=0.5, subsample=0.7;, score=-0.021 total time=   7.4s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.4, eta=0.05, gamma=1, max_depth=15, min_child_weight=5, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=0.5, reg_lambda=0.5, subsample=0.7;, score=-0.020 total time=   7.4s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.8, eta=0.1, gamma=1, max_depth=10, min_child_weight=1, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=0, reg_lambda=1.1, subsample=0.7;, score=-0.022 total time=   9.2s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.8, eta=0.1, gamma=1, max_depth=10, min_child_weight=1, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=0, reg_lambda=1.1, subsample=0.7;, score=-0.020 total time=   9.0s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.4, eta=0.2, gamma=1, max_depth=15, min_child_weight=10, n_estimators=700, objective=reg:squaredlogerror, reg_alpha=1, reg_lambda=1.3, subsample=0.7;, score=-0.021 total time=  12.1s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.4, eta=0.2, gamma=1, max_depth=15, min_child_weight=10, n_estimators=700, objective=reg:squaredlogerror, reg_alpha=1, reg_lambda=1.3, subsample=0.7;, score=-0.020 total time=  12.1s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.8, eta=0.6, gamma=1.5, max_depth=5, min_child_weight=1, n_estimators=250, objective=reg:squarederror, reg_alpha=1, reg_lambda=1.1, subsample=0.7;, score=-0.021 total time=   0.8s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.8, eta=0.6, gamma=1.5, max_depth=5, min_child_weight=1, n_estimators=250, objective=reg:squarederror, reg_alpha=1, reg_lambda=1.1, subsample=0.7;, score=-0.020 total time=   0.8s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.6, eta=0.05, gamma=2, max_depth=3, min_child_weight=5, n_estimators=700, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1.1, subsample=0.6;, score=-0.021 total time=   2.4s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.6, eta=0.05, gamma=2, max_depth=3, min_child_weight=5, n_estimators=700, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1.1, subsample=0.6;, score=-0.020 total time=   2.4s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.4, eta=0.01, gamma=1, max_depth=15, min_child_weight=1, n_estimators=400, objective=reg:squarederror, reg_alpha=1.1, reg_lambda=1, subsample=0.8;, score=-0.025 total time=   5.1s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.4, eta=0.01, gamma=1, max_depth=15, min_child_weight=1, n_estimators=400, objective=reg:squarederror, reg_alpha=1.1, reg_lambda=1, subsample=0.8;, score=-0.020 total time=   5.1s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.4, eta=0.3, gamma=1.5, max_depth=7, min_child_weight=10, n_estimators=250, objective=reg:squaredlogerror, reg_alpha=0, reg_lambda=1, subsample=0.4;, score=-0.021 total time=   1.0s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.4, eta=0.3, gamma=1.5, max_depth=7, min_child_weight=10, n_estimators=250, objective=reg:squaredlogerror, reg_alpha=0, reg_lambda=1, subsample=0.4;, score=-0.020 total time=   1.0s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.8, eta=0.2, gamma=0.5, max_depth=3, min_child_weight=1, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=1.1, reg_lambda=0.5, subsample=0.7;, score=-0.021 total time=   1.6s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.8, eta=0.2, gamma=0.5, max_depth=3, min_child_weight=1, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=1.1, reg_lambda=0.5, subsample=0.7;, score=-0.020 total time=   1.7s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.4, eta=1, gamma=2, max_depth=3, min_child_weight=5, n_estimators=1000, objective=reg:squaredlogerror, reg_alpha=1, reg_lambda=1, subsample=0.6;, score=-0.021 total time=   3.5s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.4, eta=1, gamma=2, max_depth=3, min_child_weight=5, n_estimators=1000, objective=reg:squaredlogerror, reg_alpha=1, reg_lambda=1, subsample=0.6;, score=-0.020 total time=   3.6s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=1.0, eta=0.3, gamma=5, max_depth=10, min_child_weight=5, n_estimators=400, objective=reg:squarederror, reg_alpha=1.3, reg_lambda=1.1, subsample=0.7;, score=-0.021 total time=   1.3s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=1.0, eta=0.3, gamma=5, max_depth=10, min_child_weight=5, n_estimators=400, objective=reg:squarederror, reg_alpha=1.3, reg_lambda=1.1, subsample=0.7;, score=-0.020 total time=   1.6s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.6, eta=0.1, gamma=1.5, max_depth=7, min_child_weight=5, n_estimators=250, objective=reg:squarederror, reg_alpha=1.3, reg_lambda=0.5, subsample=0.8;, score=-0.027 total time=   3.2s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.6, eta=0.1, gamma=1.5, max_depth=7, min_child_weight=5, n_estimators=250, objective=reg:squarederror, reg_alpha=1.3, reg_lambda=0.5, subsample=0.8;, score=-0.020 total time=   3.2s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=1.0, eta=0.3, gamma=1, max_depth=5, min_child_weight=1, n_estimators=1000, objective=reg:squaredlogerror, reg_alpha=0, reg_lambda=1.1, subsample=0.4;, score=-0.021 total time=   4.0s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=1.0, eta=0.3, gamma=1, max_depth=5, min_child_weight=1, n_estimators=1000, objective=reg:squaredlogerror, reg_alpha=0, reg_lambda=1.1, subsample=0.4;, score=-0.020 total time=   4.1s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=1.0, eta=0.2, gamma=1.5, max_depth=3, min_child_weight=10, n_estimators=700, objective=reg:squaredlogerror, reg_alpha=1.3, reg_lambda=1.3, subsample=0.7;, score=-0.021 total time=   2.6s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=1.0, eta=0.2, gamma=1.5, max_depth=3, min_child_weight=10, n_estimators=700, objective=reg:squaredlogerror, reg_alpha=1.3, reg_lambda=1.3, subsample=0.7;, score=-0.020 total time=   2.6s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=1.0, eta=0.05, gamma=2, max_depth=5, min_child_weight=10, n_estimators=1000, objective=reg:squaredlogerror, reg_alpha=1, reg_lambda=0.5, subsample=0.8;, score=-0.021 total time=   3.6s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=1.0, eta=0.05, gamma=2, max_depth=5, min_child_weight=10, n_estimators=1000, objective=reg:squaredlogerror, reg_alpha=1, reg_lambda=0.5, subsample=0.8;, score=-0.020 total time=   3.5s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=1.0, eta=0.3, gamma=1.5, max_depth=7, min_child_weight=1, n_estimators=1000, objective=reg:squaredlogerror, reg_alpha=0.5, reg_lambda=1.3, subsample=0.6;, score=-0.021 total time=   3.5s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=1.0, eta=0.3, gamma=1.5, max_depth=7, min_child_weight=1, n_estimators=1000, objective=reg:squaredlogerror, reg_alpha=0.5, reg_lambda=1.3, subsample=0.6;, score=-0.020 total time=   3.5s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=1.0, eta=1, gamma=0.5, max_depth=10, min_child_weight=10, n_estimators=400, objective=reg:squarederror, reg_alpha=0, reg_lambda=1.1, subsample=0.6;, score=-0.021 total time=   1.7s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=1.0, eta=1, gamma=0.5, max_depth=10, min_child_weight=10, n_estimators=400, objective=reg:squarederror, reg_alpha=0, reg_lambda=1.1, subsample=0.6;, score=-0.020 total time=   1.8s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.4, eta=0.05, gamma=1, max_depth=10, min_child_weight=1, n_estimators=250, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1.3, subsample=0.4;, score=-0.021 total time=   0.8s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.4, eta=0.05, gamma=1, max_depth=10, min_child_weight=1, n_estimators=250, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1.3, subsample=0.4;, score=-0.020 total time=   0.8s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.4, eta=1, gamma=1, max_depth=7, min_child_weight=5, n_estimators=400, objective=reg:squarederror, reg_alpha=0, reg_lambda=1.1, subsample=0.6;, score=-0.061 total time=   4.7s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.4, eta=1, gamma=1, max_depth=7, min_child_weight=5, n_estimators=400, objective=reg:squarederror, reg_alpha=0, reg_lambda=1.1, subsample=0.6;, score=-0.042 total time=   4.7s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.8, eta=0.2, gamma=5, max_depth=3, min_child_weight=10, n_estimators=250, objective=reg:squaredlogerror, reg_alpha=0, reg_lambda=1.3, subsample=0.7;, score=-0.021 total time=   2.2s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.8, eta=0.2, gamma=5, max_depth=3, min_child_weight=10, n_estimators=250, objective=reg:squaredlogerror, reg_alpha=0, reg_lambda=1.3, subsample=0.7;, score=-0.020 total time=   2.3s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.4, eta=0.6, gamma=2, max_depth=5, min_child_weight=10, n_estimators=1000, objective=reg:squaredlogerror, reg_alpha=1.1, reg_lambda=1, subsample=0.6;, score=-0.021 total time=   3.5s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.4, eta=0.6, gamma=2, max_depth=5, min_child_weight=10, n_estimators=1000, objective=reg:squaredlogerror, reg_alpha=1.1, reg_lambda=1, subsample=0.6;, score=-0.020 total time=   3.6s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=1.0, eta=0.1, gamma=1.5, max_depth=5, min_child_weight=10, n_estimators=700, objective=reg:squaredlogerror, reg_alpha=1.3, reg_lambda=1.3, subsample=0.4;, score=-0.021 total time=   7.8s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=1.0, eta=0.1, gamma=1.5, max_depth=5, min_child_weight=10, n_estimators=700, objective=reg:squaredlogerror, reg_alpha=1.3, reg_lambda=1.3, subsample=0.4;, score=-0.020 total time=   7.8s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.8, eta=1, gamma=2, max_depth=7, min_child_weight=5, n_estimators=400, objective=reg:squarederror, reg_alpha=1.1, reg_lambda=1, subsample=0.4;, score=-0.044 total time=   6.1s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.8, eta=1, gamma=2, max_depth=7, min_child_weight=5, n_estimators=400, objective=reg:squarederror, reg_alpha=1.1, reg_lambda=1, subsample=0.4;, score=-0.030 total time=   6.1s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.8, eta=0.2, gamma=1, max_depth=10, min_child_weight=10, n_estimators=400, objective=reg:squarederror, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=-0.036 total time=   8.8s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.8, eta=0.2, gamma=1, max_depth=10, min_child_weight=10, n_estimators=400, objective=reg:squarederror, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=-0.025 total time=   9.0s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=1.0, eta=0.3, gamma=2, max_depth=5, min_child_weight=10, n_estimators=400, objective=reg:squarederror, reg_alpha=1, reg_lambda=1, subsample=0.7;, score=-0.021 total time=   1.4s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=1.0, eta=0.3, gamma=2, max_depth=5, min_child_weight=10, n_estimators=400, objective=reg:squarederror, reg_alpha=1, reg_lambda=1, subsample=0.7;, score=-0.020 total time=   1.6s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.4, eta=0.05, gamma=1, max_depth=3, min_child_weight=1, n_estimators=1000, objective=reg:squaredlogerror, reg_alpha=1, reg_lambda=1.3, subsample=0.6;, score=-0.021 total time=   6.7s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.4, eta=0.05, gamma=1, max_depth=3, min_child_weight=1, n_estimators=1000, objective=reg:squaredlogerror, reg_alpha=1, reg_lambda=1.3, subsample=0.6;, score=-0.020 total time=   6.6s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.8, eta=0.05, gamma=1.5, max_depth=5, min_child_weight=1, n_estimators=700, objective=reg:squarederror, reg_alpha=1, reg_lambda=1, subsample=0.4;, score=-0.035 total time=   7.5s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.8, eta=0.05, gamma=1.5, max_depth=5, min_child_weight=1, n_estimators=700, objective=reg:squarederror, reg_alpha=1, reg_lambda=1, subsample=0.4;, score=-0.021 total time=   7.5s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.4, eta=0.05, gamma=1.5, max_depth=15, min_child_weight=10, n_estimators=250, objective=reg:squarederror, reg_alpha=1, reg_lambda=1, subsample=0.4;, score=-0.021 total time=   0.9s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.4, eta=0.05, gamma=1.5, max_depth=15, min_child_weight=10, n_estimators=250, objective=reg:squarederror, reg_alpha=1, reg_lambda=1, subsample=0.4;, score=-0.020 total time=   0.8s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=1.0, eta=0.3, gamma=5, max_depth=7, min_child_weight=10, n_estimators=700, objective=reg:squaredlogerror, reg_alpha=1, reg_lambda=1.1, subsample=0.8;, score=-0.021 total time=   2.7s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=1.0, eta=0.3, gamma=5, max_depth=7, min_child_weight=10, n_estimators=700, objective=reg:squaredlogerror, reg_alpha=1, reg_lambda=1.1, subsample=0.8;, score=-0.020 total time=   2.6s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.4, eta=0.3, gamma=0.5, max_depth=15, min_child_weight=1, n_estimators=700, objective=reg:squarederror, reg_alpha=1.1, reg_lambda=1.1, subsample=0.7;, score=-0.037 total time=  16.1s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.4, eta=0.3, gamma=0.5, max_depth=15, min_child_weight=1, n_estimators=700, objective=reg:squarederror, reg_alpha=1.1, reg_lambda=1.1, subsample=0.7;, score=-0.030 total time=  16.1s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.6, eta=0.1, gamma=1, max_depth=15, min_child_weight=1, n_estimators=700, objective=reg:squaredlogerror, reg_alpha=1.3, reg_lambda=1.1, subsample=0.8;, score=-0.021 total time=  14.5s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.6, eta=0.1, gamma=1, max_depth=15, min_child_weight=1, n_estimators=700, objective=reg:squaredlogerror, reg_alpha=1.3, reg_lambda=1.1, subsample=0.8;, score=-0.020 total time=  14.9s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.6, eta=0.1, gamma=2, max_depth=15, min_child_weight=1, n_estimators=250, objective=reg:squarederror, reg_alpha=1.3, reg_lambda=1.3, subsample=0.7;, score=-0.030 total time=   6.5s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.6, eta=0.1, gamma=2, max_depth=15, min_child_weight=1, n_estimators=250, objective=reg:squarederror, reg_alpha=1.3, reg_lambda=1.3, subsample=0.7;, score=-0.021 total time=   6.6s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.8, eta=0.2, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=250, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1.1, subsample=0.8;, score=-0.021 total time=   0.9s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.8, eta=0.2, gamma=0.5, max_depth=3, min_child_weight=5, n_estimators=250, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1.1, subsample=0.8;, score=-0.020 total time=   0.8s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.4, eta=0.01, gamma=1.5, max_depth=3, min_child_weight=10, n_estimators=1000, objective=reg:squaredlogerror, reg_alpha=0.5, reg_lambda=1.1, subsample=0.8;, score=-0.021 total time=   3.6s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.4, eta=0.01, gamma=1.5, max_depth=3, min_child_weight=10, n_estimators=1000, objective=reg:squaredlogerror, reg_alpha=0.5, reg_lambda=1.1, subsample=0.8;, score=-0.020 total time=   3.5s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.4, eta=1, gamma=5, max_depth=3, min_child_weight=1, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=0.5, reg_lambda=1, subsample=0.4;, score=-0.021 total time=   2.8s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.4, eta=1, gamma=5, max_depth=3, min_child_weight=1, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=0.5, reg_lambda=1, subsample=0.4;, score=-0.020 total time=   2.8s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.8, eta=0.05, gamma=1, max_depth=7, min_child_weight=10, n_estimators=250, objective=reg:squaredlogerror, reg_alpha=1.1, reg_lambda=0.5, subsample=0.8;, score=-0.021 total time=   0.9s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.8, eta=0.05, gamma=1, max_depth=7, min_child_weight=10, n_estimators=250, objective=reg:squaredlogerror, reg_alpha=1.1, reg_lambda=0.5, subsample=0.8;, score=-0.020 total time=   0.9s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.8, eta=0.3, gamma=5, max_depth=7, min_child_weight=1, n_estimators=400, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1.3, subsample=0.4;, score=-0.029 total time=   5.9s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.8, eta=0.3, gamma=5, max_depth=7, min_child_weight=1, n_estimators=400, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1.3, subsample=0.4;, score=-0.021 total time=   5.8s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.6, eta=0.2, gamma=1.5, max_depth=3, min_child_weight=5, n_estimators=250, objective=reg:squarederror, reg_alpha=1.1, reg_lambda=1, subsample=0.4;, score=-0.026 total time=   2.0s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.6, eta=0.2, gamma=1.5, max_depth=3, min_child_weight=5, n_estimators=250, objective=reg:squarederror, reg_alpha=1.1, reg_lambda=1, subsample=0.4;, score=-0.020 total time=   2.0s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=1.0, eta=0.01, gamma=0.5, max_depth=5, min_child_weight=1, n_estimators=700, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1.1, subsample=0.7;, score=-0.030 total time=   7.8s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=1.0, eta=0.01, gamma=0.5, max_depth=5, min_child_weight=1, n_estimators=700, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1.1, subsample=0.7;, score=-0.020 total time=   7.8s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.4, eta=0.1, gamma=0.5, max_depth=15, min_child_weight=10, n_estimators=1000, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1.3, subsample=0.7;, score=-0.031 total time=  23.7s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.4, eta=0.1, gamma=0.5, max_depth=15, min_child_weight=10, n_estimators=1000, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1.3, subsample=0.7;, score=-0.024 total time=  24.7s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=1.0, eta=0.6, gamma=2, max_depth=15, min_child_weight=10, n_estimators=1000, objective=reg:squaredlogerror, reg_alpha=0, reg_lambda=1.1, subsample=0.6;, score=-0.021 total time=   4.2s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=1.0, eta=0.6, gamma=2, max_depth=15, min_child_weight=10, n_estimators=1000, objective=reg:squaredlogerror, reg_alpha=0, reg_lambda=1.1, subsample=0.6;, score=-0.020 total time=   4.1s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=1.0, eta=0.6, gamma=5, max_depth=15, min_child_weight=1, n_estimators=700, objective=reg:squarederror, reg_alpha=1, reg_lambda=0.5, subsample=0.6;, score=-0.021 total time=   2.5s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=1.0, eta=0.6, gamma=5, max_depth=15, min_child_weight=1, n_estimators=700, objective=reg:squarederror, reg_alpha=1, reg_lambda=0.5, subsample=0.6;, score=-0.020 total time=   2.5s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.8, eta=0.3, gamma=0.5, max_depth=5, min_child_weight=1, n_estimators=250, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1.3, subsample=0.7;, score=-0.051 total time=   3.0s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.8, eta=0.3, gamma=0.5, max_depth=5, min_child_weight=1, n_estimators=250, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1.3, subsample=0.7;, score=-0.033 total time=   3.0s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.4, eta=0.01, gamma=5, max_depth=7, min_child_weight=10, n_estimators=700, objective=reg:squaredlogerror, reg_alpha=1.3, reg_lambda=1.1, subsample=0.6;, score=-0.021 total time=   6.4s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.4, eta=0.01, gamma=5, max_depth=7, min_child_weight=10, n_estimators=700, objective=reg:squaredlogerror, reg_alpha=1.3, reg_lambda=1.1, subsample=0.6;, score=-0.020 total time=   6.4s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=1.0, eta=0.3, gamma=2, max_depth=10, min_child_weight=5, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=0.5, reg_lambda=1.1, subsample=0.8;, score=-0.021 total time=   1.7s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=1.0, eta=0.3, gamma=2, max_depth=10, min_child_weight=5, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=0.5, reg_lambda=1.1, subsample=0.8;, score=-0.020 total time=   1.5s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.4, eta=0.1, gamma=0.5, max_depth=3, min_child_weight=1, n_estimators=400, objective=reg:squarederror, reg_alpha=0, reg_lambda=0.5, subsample=0.4;, score=-0.046 total time=   2.7s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.4, eta=0.1, gamma=0.5, max_depth=3, min_child_weight=1, n_estimators=400, objective=reg:squarederror, reg_alpha=0, reg_lambda=0.5, subsample=0.4;, score=-0.024 total time=   2.9s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.4, eta=0.2, gamma=2, max_depth=7, min_child_weight=5, n_estimators=1000, objective=reg:squarederror, reg_alpha=1, reg_lambda=1, subsample=0.8;, score=-0.021 total time=   3.4s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.4, eta=0.2, gamma=2, max_depth=7, min_child_weight=5, n_estimators=1000, objective=reg:squarederror, reg_alpha=1, reg_lambda=1, subsample=0.8;, score=-0.020 total time=   3.3s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.8, eta=0.05, gamma=0.5, max_depth=3, min_child_weight=1, n_estimators=700, objective=reg:squarederror, reg_alpha=0, reg_lambda=0.5, subsample=0.4;, score=-0.052 total time=   5.5s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.8, eta=0.05, gamma=0.5, max_depth=3, min_child_weight=1, n_estimators=700, objective=reg:squarederror, reg_alpha=0, reg_lambda=0.5, subsample=0.4;, score=-0.021 total time=   5.5s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.6, eta=0.6, gamma=1.5, max_depth=10, min_child_weight=1, n_estimators=1000, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1.1, subsample=0.8;, score=-0.073 total time=  19.8s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.6, eta=0.6, gamma=1.5, max_depth=10, min_child_weight=1, n_estimators=1000, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1.1, subsample=0.8;, score=-0.035 total time=  19.9s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.4, eta=0.2, gamma=1.5, max_depth=10, min_child_weight=5, n_estimators=1000, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1, subsample=0.4;, score=-0.021 total time=   3.3s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.4, eta=0.2, gamma=1.5, max_depth=10, min_child_weight=5, n_estimators=1000, objective=reg:squarederror, reg_alpha=0.5, reg_lambda=1, subsample=0.4;, score=-0.020 total time=   3.3s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.8, eta=0.2, gamma=2, max_depth=15, min_child_weight=1, n_estimators=1000, objective=reg:squarederror, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=-0.076 total time=  53.6s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.8, eta=0.2, gamma=2, max_depth=15, min_child_weight=1, n_estimators=1000, objective=reg:squarederror, reg_alpha=0, reg_lambda=1, subsample=0.7;, score=-0.033 total time=  55.2s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.4, eta=0.1, gamma=0.5, max_depth=10, min_child_weight=1, n_estimators=400, objective=reg:squarederror, reg_alpha=0, reg_lambda=1, subsample=0.4;, score=-0.021 total time=   1.7s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.4, eta=0.1, gamma=0.5, max_depth=10, min_child_weight=1, n_estimators=400, objective=reg:squarederror, reg_alpha=0, reg_lambda=1, subsample=0.4;, score=-0.020 total time=   1.7s\n",
      "[CV 1/2] END booster=gbtree, colsample_bytree=0.8, eta=0.3, gamma=0.5, max_depth=10, min_child_weight=5, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=1.1, reg_lambda=0.5, subsample=0.4;, score=-0.022 total time=   7.7s\n",
      "[CV 2/2] END booster=gbtree, colsample_bytree=0.8, eta=0.3, gamma=0.5, max_depth=10, min_child_weight=5, n_estimators=400, objective=reg:squaredlogerror, reg_alpha=1.1, reg_lambda=0.5, subsample=0.4;, score=-0.020 total time=   7.7s\n",
      "[CV 1/2] END booster=gblinear, colsample_bytree=0.4, eta=0.6, gamma=1, max_depth=7, min_child_weight=5, n_estimators=1000, objective=reg:squarederror, reg_alpha=1.3, reg_lambda=0.5, subsample=0.8;, score=-0.021 total time=   3.3s\n",
      "[CV 2/2] END booster=gblinear, colsample_bytree=0.4, eta=0.6, gamma=1, max_depth=7, min_child_weight=5, n_estimators=1000, objective=reg:squarederror, reg_alpha=1.3, reg_lambda=0.5, subsample=0.8;, score=-0.020 total time=   3.2s\n",
      "18.362452733516694 minutes.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Select random cluster and tune hyperparameters on it\n",
    "# Alternative would be to tune hyperparameters for every single model, but that would be very time consuming\n",
    "selected_cluster = prepare_cluster_data(data, random.randint(1, 3), random.randint(1, 100), True)\n",
    "X_train, y_train, X_test, y_test = test_train_yearly_split(selected_cluster)\n",
    "\n",
    "while(len(y_train == 0)):\n",
    "    selected_cluster = prepare_cluster_data(data, random.randint(1, 3), random.randint(1, 100), True)\n",
    "    X_train, y_train, X_test, y_test = test_train_yearly_split(selected_cluster)\n",
    "\n",
    "\n",
    "xgb_r = XGBRegressor(\n",
    "        seed=29,\n",
    "        verbosity=0)\n",
    "model = hyperparameter_tuning(X_train, X_test, y_train, y_test, xgb_r, parameters_XGB, cv=5, n_iterations=100)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"{(end-start)/60} minutes.\")\n",
    "best_params_xgb = model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af06c4a4-2ce9-48a6-9751-c0a635b5610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(verbosity=0, \n",
    "                   subsample=best_params_xgb['subsample'],\n",
    "                   seed=29,\n",
    "                   reg_lambda=best_params_xgb['reg_lambda'],\n",
    "                   reg_alpha=best_params_xgb['reg_alpha'],\n",
    "                   objective=best_params_xgb['objective'],\n",
    "                   n_estimators=best_params_xgb['n_estimators'],\n",
    "                   min_child_weight=best_params_xgb['min_child_weight'],\n",
    "                   max_depth=best_params_xgb['max_depth'],\n",
    "                   gamma=best_params_xgb['gamma'],\n",
    "                   eta=best_params_xgb['eta'],\n",
    "                   colsample_bytree=best_params_xgb['colsample_bytree'],\n",
    "                   booster=best_params_xgb['booster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26c9d9c1-b643-4673-a31c-bfd32e696378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluated_clusters_xgb = {}\n",
    "\n",
    "for kmns in range(1, 101):\n",
    "    for hier in range(1, 4):\n",
    "        try:\n",
    "            key, = run_evaluate_model(data, xgb, hier, kmns, False).items()\n",
    "            evaluated_clusters_xgb[key[0]] = key[1]\n",
    "        except:\n",
    "            # Some of the clusters only contain last year of sales, so we can skip those\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a741087-6444-4b08-b5f4-4e282b0434e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_score_xgb = []\n",
    "for kmns in range(1, 101):\n",
    "        for hier in range(1, 4):\n",
    "            try:\n",
    "                total_score_xgb.append(evaluated_clusters_xgb[f'kmeans-{kmns}_hierarchical-{hier}']['test']['total'])\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c1cea6d-f849-4f54-9f95-d114d0781482",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('xgb_evaluated.json', 'w') as outfile:\n",
    "    json.dump(evaluated_clusters_xgb, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c61bd7de-5339-480c-8121-4339744708b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 1 out of 10 models. (10.0%)\n",
      "Done 2 out of 10 models. (20.0%)\n",
      "Done 3 out of 10 models. (30.0%)\n",
      "Done 4 out of 10 models. (40.0%)\n",
      "Done 5 out of 10 models. (50.0%)\n",
      "Done 6 out of 10 models. (60.0%)\n",
      "Done 7 out of 10 models. (70.0%)\n",
      "Done 8 out of 10 models. (80.0%)\n",
      "Done 9 out of 10 models. (90.0%)\n",
      "Done 10 out of 10 models. (100.0%)\n"
     ]
    }
   ],
   "source": [
    "importances_xgb = feature_importance_xgb(data, TEST_SIZE=10, model=xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d9b176-d8c5-4e0e-b74d-89df0b3b7ea7",
   "metadata": {},
   "source": [
    "# CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9312b59e-6a4e-4ab0-8d7d-59d6c5fb68ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_CB = {'max_depth' : [5,8,10],\n",
    "              'learning_rate' : [0.01, 0.02, 0.05, 0.1],\n",
    "              'iterations'    : [50, 100, 250, 500],\n",
    "              'l2_leaf_reg'   : [5, 10, 20]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c9e463c-2ebd-4d82-83d1-f26a5d5706b9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
      "[CV 1/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=10;, score=-0.020 total time=   7.0s\n",
      "[CV 2/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=10;, score=-0.021 total time=   5.3s\n",
      "[CV 3/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=10;, score=-0.028 total time=   6.2s\n",
      "[CV 4/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=10;, score=-0.017 total time=   5.3s\n",
      "[CV 5/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=10;, score=-0.016 total time=   4.7s\n",
      "[CV 6/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=10;, score=-0.023 total time=   4.9s\n",
      "[CV 7/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=10;, score=-0.018 total time=   5.7s\n",
      "[CV 8/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=10;, score=-0.020 total time=   6.5s\n",
      "[CV 9/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=10;, score=-0.021 total time=   6.7s\n",
      "[CV 10/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=10;, score=-0.014 total time=   6.2s\n",
      "[CV 1/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.020 total time=   1.5s\n",
      "[CV 2/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.020 total time=   0.6s\n",
      "[CV 3/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.029 total time=   0.8s\n",
      "[CV 4/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.018 total time=   0.7s\n",
      "[CV 5/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.016 total time=   1.5s\n",
      "[CV 6/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.023 total time=   0.7s\n",
      "[CV 7/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.019 total time=   2.1s\n",
      "[CV 8/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.020 total time=   1.3s\n",
      "[CV 9/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.021 total time=   1.5s\n",
      "[CV 10/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.014 total time=   1.1s\n",
      "[CV 1/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.020 total time=   2.2s\n",
      "[CV 2/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.020 total time=   1.4s\n",
      "[CV 3/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.029 total time=   1.4s\n",
      "[CV 4/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.018 total time=   2.4s\n",
      "[CV 5/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.016 total time=   1.4s\n",
      "[CV 6/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.024 total time=   1.3s\n",
      "[CV 7/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.019 total time=   2.4s\n",
      "[CV 8/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.020 total time=   1.6s\n",
      "[CV 9/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.021 total time=   1.4s\n",
      "[CV 10/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.014 total time=   1.2s\n",
      "[CV 1/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.01, max_depth=5;, score=-0.020 total time=   1.5s\n",
      "[CV 2/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.01, max_depth=5;, score=-0.020 total time=   2.9s\n",
      "[CV 3/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.01, max_depth=5;, score=-0.029 total time=   1.1s\n",
      "[CV 4/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.01, max_depth=5;, score=-0.018 total time=   1.3s\n",
      "[CV 5/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.01, max_depth=5;, score=-0.016 total time=   1.2s\n",
      "[CV 6/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.01, max_depth=5;, score=-0.023 total time=   1.2s\n",
      "[CV 7/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.01, max_depth=5;, score=-0.019 total time=   1.2s\n",
      "[CV 8/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.01, max_depth=5;, score=-0.020 total time=   1.4s\n",
      "[CV 9/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.01, max_depth=5;, score=-0.021 total time=   2.4s\n",
      "[CV 10/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.01, max_depth=5;, score=-0.014 total time=   1.4s\n",
      "[CV 1/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.1, max_depth=5;, score=-0.022 total time=   1.4s\n",
      "[CV 2/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.1, max_depth=5;, score=-0.021 total time=   1.2s\n",
      "[CV 3/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.1, max_depth=5;, score=-0.029 total time=   1.5s\n",
      "[CV 4/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.1, max_depth=5;, score=-0.018 total time=   1.6s\n",
      "[CV 5/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.1, max_depth=5;, score=-0.017 total time=   1.6s\n",
      "[CV 6/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.1, max_depth=5;, score=-0.023 total time=   2.3s\n",
      "[CV 7/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.1, max_depth=5;, score=-0.020 total time=   1.4s\n",
      "[CV 8/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.1, max_depth=5;, score=-0.020 total time=   1.4s\n",
      "[CV 9/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.1, max_depth=5;, score=-0.021 total time=   2.9s\n",
      "[CV 10/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.1, max_depth=5;, score=-0.015 total time=   1.3s\n",
      "[CV 1/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.1, max_depth=10;, score=-0.020 total time=   1.4s\n",
      "[CV 2/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.1, max_depth=10;, score=-0.020 total time=   2.4s\n",
      "[CV 3/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.1, max_depth=10;, score=-0.028 total time=   1.4s\n",
      "[CV 4/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.1, max_depth=10;, score=-0.017 total time=   2.4s\n",
      "[CV 5/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.1, max_depth=10;, score=-0.016 total time=   1.4s\n",
      "[CV 6/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.1, max_depth=10;, score=-0.023 total time=   1.8s\n",
      "[CV 7/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.1, max_depth=10;, score=-0.018 total time=   1.4s\n",
      "[CV 8/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.1, max_depth=10;, score=-0.020 total time=   3.0s\n",
      "[CV 9/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.1, max_depth=10;, score=-0.021 total time=   1.4s\n",
      "[CV 10/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.1, max_depth=10;, score=-0.014 total time=   2.5s\n",
      "[CV 1/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.020 total time=   3.7s\n",
      "[CV 2/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.020 total time=   2.4s\n",
      "[CV 3/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.029 total time=   3.0s\n",
      "[CV 4/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.018 total time=   2.2s\n",
      "[CV 5/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.016 total time=   3.1s\n",
      "[CV 6/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.023 total time=   2.2s\n",
      "[CV 7/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.019 total time=   2.3s\n",
      "[CV 8/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.020 total time=   2.4s\n",
      "[CV 9/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.021 total time=   2.3s\n",
      "[CV 10/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.014 total time=   3.1s\n",
      "[CV 1/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=10;, score=-0.020 total time=   2.1s\n",
      "[CV 2/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=10;, score=-0.020 total time=   1.3s\n",
      "[CV 3/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=10;, score=-0.029 total time=   1.9s\n",
      "[CV 4/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=10;, score=-0.018 total time=   1.4s\n",
      "[CV 5/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=10;, score=-0.016 total time=   2.2s\n",
      "[CV 6/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=10;, score=-0.023 total time=   1.4s\n",
      "[CV 7/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=10;, score=-0.019 total time=   1.3s\n",
      "[CV 8/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=10;, score=-0.020 total time=   1.4s\n",
      "[CV 9/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=10;, score=-0.021 total time=   2.1s\n",
      "[CV 10/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=10;, score=-0.014 total time=   2.6s\n",
      "[CV 1/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.05, max_depth=10;, score=-0.020 total time=   1.8s\n",
      "[CV 2/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.05, max_depth=10;, score=-0.020 total time=   1.3s\n",
      "[CV 3/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.05, max_depth=10;, score=-0.029 total time=   1.8s\n",
      "[CV 4/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.05, max_depth=10;, score=-0.017 total time=   1.7s\n",
      "[CV 5/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.05, max_depth=10;, score=-0.016 total time=   2.0s\n",
      "[CV 6/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.05, max_depth=10;, score=-0.023 total time=   1.5s\n",
      "[CV 7/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.05, max_depth=10;, score=-0.018 total time=   1.5s\n",
      "[CV 8/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.05, max_depth=10;, score=-0.020 total time=   1.4s\n",
      "[CV 9/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.05, max_depth=10;, score=-0.021 total time=   2.0s\n",
      "[CV 10/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.05, max_depth=10;, score=-0.014 total time=   1.4s\n",
      "[CV 1/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.05, max_depth=5;, score=-0.020 total time=   1.8s\n",
      "[CV 2/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.05, max_depth=5;, score=-0.020 total time=   0.7s\n",
      "[CV 3/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.05, max_depth=5;, score=-0.029 total time=   0.7s\n",
      "[CV 4/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.05, max_depth=5;, score=-0.018 total time=   0.7s\n",
      "[CV 5/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.05, max_depth=5;, score=-0.016 total time=   2.1s\n",
      "[CV 6/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.05, max_depth=5;, score=-0.023 total time=   0.8s\n",
      "[CV 7/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.05, max_depth=5;, score=-0.019 total time=   1.0s\n",
      "[CV 8/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.05, max_depth=5;, score=-0.020 total time=   2.3s\n",
      "[CV 9/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.05, max_depth=5;, score=-0.021 total time=   0.8s\n",
      "[CV 10/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.05, max_depth=5;, score=-0.014 total time=   0.9s\n",
      "[CV 1/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.020 total time=   2.9s\n",
      "[CV 2/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.020 total time=   1.5s\n",
      "[CV 3/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.029 total time=   1.4s\n",
      "[CV 4/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.018 total time=   1.3s\n",
      "[CV 5/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.016 total time=   1.8s\n",
      "[CV 6/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.023 total time=   1.6s\n",
      "[CV 7/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.019 total time=   2.7s\n",
      "[CV 8/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.020 total time=   1.3s\n",
      "[CV 9/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.021 total time=   1.9s\n",
      "[CV 10/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.014 total time=   1.9s\n",
      "[CV 1/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.05, max_depth=10;, score=-0.020 total time=   4.6s\n",
      "[CV 2/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.05, max_depth=10;, score=-0.020 total time=   2.6s\n",
      "[CV 3/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.05, max_depth=10;, score=-0.028 total time=   2.8s\n",
      "[CV 4/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.05, max_depth=10;, score=-0.017 total time=   3.5s\n",
      "[CV 5/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.05, max_depth=10;, score=-0.016 total time=   2.3s\n",
      "[CV 6/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.05, max_depth=10;, score=-0.023 total time=   2.5s\n",
      "[CV 7/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.05, max_depth=10;, score=-0.018 total time=   3.4s\n",
      "[CV 8/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.05, max_depth=10;, score=-0.020 total time=   2.5s\n",
      "[CV 9/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.05, max_depth=10;, score=-0.021 total time=   2.3s\n",
      "[CV 10/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.05, max_depth=10;, score=-0.014 total time=   3.0s\n",
      "[CV 1/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.02, max_depth=10;, score=-0.020 total time=   2.1s\n",
      "[CV 2/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.02, max_depth=10;, score=-0.020 total time=   1.6s\n",
      "[CV 3/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.02, max_depth=10;, score=-0.029 total time=   2.6s\n",
      "[CV 4/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.02, max_depth=10;, score=-0.018 total time=   1.2s\n",
      "[CV 5/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.02, max_depth=10;, score=-0.016 total time=   1.4s\n",
      "[CV 6/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.02, max_depth=10;, score=-0.023 total time=   1.3s\n",
      "[CV 7/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.02, max_depth=10;, score=-0.019 total time=   1.4s\n",
      "[CV 8/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.02, max_depth=10;, score=-0.020 total time=   2.4s\n",
      "[CV 9/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.02, max_depth=10;, score=-0.021 total time=   1.9s\n",
      "[CV 10/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.02, max_depth=10;, score=-0.014 total time=   3.7s\n",
      "[CV 1/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.020 total time=   6.6s\n",
      "[CV 2/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.020 total time=   6.9s\n",
      "[CV 3/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.029 total time=   5.4s\n",
      "[CV 4/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.017 total time=   7.5s\n",
      "[CV 5/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.016 total time=   5.5s\n",
      "[CV 6/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.023 total time=   5.5s\n",
      "[CV 7/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.019 total time=   5.7s\n",
      "[CV 8/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.020 total time=   5.1s\n",
      "[CV 9/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.021 total time=   5.4s\n",
      "[CV 10/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.01, max_depth=10;, score=-0.014 total time=   6.6s\n",
      "[CV 1/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.01, max_depth=5;, score=-0.020 total time=   3.0s\n",
      "[CV 2/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.01, max_depth=5;, score=-0.020 total time=   1.6s\n",
      "[CV 3/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.01, max_depth=5;, score=-0.029 total time=   4.1s\n",
      "[CV 4/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.01, max_depth=5;, score=-0.018 total time=   2.1s\n",
      "[CV 5/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.01, max_depth=5;, score=-0.016 total time=   1.8s\n",
      "[CV 6/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.01, max_depth=5;, score=-0.024 total time=   1.7s\n",
      "[CV 7/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.01, max_depth=5;, score=-0.019 total time=   0.9s\n",
      "[CV 8/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.01, max_depth=5;, score=-0.020 total time=   1.6s\n",
      "[CV 9/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.01, max_depth=5;, score=-0.021 total time=   0.9s\n",
      "[CV 10/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.01, max_depth=5;, score=-0.014 total time=   3.6s\n",
      "[CV 1/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=8;, score=-0.020 total time=   3.0s\n",
      "[CV 2/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=8;, score=-0.021 total time=   2.3s\n",
      "[CV 3/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=8;, score=-0.029 total time=   2.0s\n",
      "[CV 4/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=8;, score=-0.017 total time=   3.2s\n",
      "[CV 5/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=8;, score=-0.016 total time=   3.8s\n",
      "[CV 6/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=8;, score=-0.023 total time=   3.0s\n",
      "[CV 7/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=8;, score=-0.018 total time=   2.1s\n",
      "[CV 8/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=8;, score=-0.020 total time=   3.7s\n",
      "[CV 9/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=8;, score=-0.021 total time=   2.4s\n",
      "[CV 10/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=8;, score=-0.014 total time=   2.5s\n",
      "[CV 1/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.01, max_depth=5;, score=-0.020 total time=   1.7s\n",
      "[CV 2/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.01, max_depth=5;, score=-0.020 total time=   1.2s\n",
      "[CV 3/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.01, max_depth=5;, score=-0.029 total time=   3.1s\n",
      "[CV 4/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.01, max_depth=5;, score=-0.018 total time=   1.6s\n",
      "[CV 5/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.01, max_depth=5;, score=-0.016 total time=   1.3s\n",
      "[CV 6/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.01, max_depth=5;, score=-0.023 total time=   1.5s\n",
      "[CV 7/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.01, max_depth=5;, score=-0.019 total time=   1.8s\n",
      "[CV 8/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.01, max_depth=5;, score=-0.020 total time=   1.6s\n",
      "[CV 9/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.01, max_depth=5;, score=-0.021 total time=   1.7s\n",
      "[CV 10/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.01, max_depth=5;, score=-0.014 total time=   1.3s\n",
      "[CV 1/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=10;, score=-0.020 total time=   3.2s\n",
      "[CV 2/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=10;, score=-0.020 total time=   2.7s\n",
      "[CV 3/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=10;, score=-0.028 total time=   3.0s\n",
      "[CV 4/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=10;, score=-0.017 total time=   2.4s\n",
      "[CV 5/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=10;, score=-0.016 total time=   3.8s\n",
      "[CV 6/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=10;, score=-0.023 total time=   2.5s\n",
      "[CV 7/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=10;, score=-0.018 total time=   2.4s\n",
      "[CV 8/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=10;, score=-0.020 total time=   3.2s\n",
      "[CV 9/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=10;, score=-0.021 total time=   2.7s\n",
      "[CV 10/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=10;, score=-0.014 total time=   2.4s\n",
      "[CV 1/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.05, max_depth=5;, score=-0.020 total time=   0.7s\n",
      "[CV 2/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.05, max_depth=5;, score=-0.020 total time=   0.5s\n",
      "[CV 3/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.05, max_depth=5;, score=-0.029 total time=   0.6s\n",
      "[CV 4/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.05, max_depth=5;, score=-0.018 total time=   0.7s\n",
      "[CV 5/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.05, max_depth=5;, score=-0.016 total time=   0.7s\n",
      "[CV 6/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.05, max_depth=5;, score=-0.023 total time=   2.7s\n",
      "[CV 7/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.05, max_depth=5;, score=-0.019 total time=   3.5s\n",
      "[CV 8/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.05, max_depth=5;, score=-0.020 total time=   3.2s\n",
      "[CV 9/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.05, max_depth=5;, score=-0.021 total time=   3.6s\n",
      "[CV 10/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.05, max_depth=5;, score=-0.014 total time=   4.9s\n",
      "[CV 1/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.1, max_depth=8;, score=-0.020 total time=   2.7s\n",
      "[CV 2/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.1, max_depth=8;, score=-0.021 total time=   1.9s\n",
      "[CV 3/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.1, max_depth=8;, score=-0.029 total time=   2.1s\n",
      "[CV 4/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.1, max_depth=8;, score=-0.018 total time=   1.6s\n",
      "[CV 5/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.1, max_depth=8;, score=-0.016 total time=   3.1s\n",
      "[CV 6/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.1, max_depth=8;, score=-0.023 total time=   1.5s\n",
      "[CV 7/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.1, max_depth=8;, score=-0.018 total time=   2.2s\n",
      "[CV 8/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.1, max_depth=8;, score=-0.020 total time=   1.9s\n",
      "[CV 9/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.1, max_depth=8;, score=-0.021 total time=   2.3s\n",
      "[CV 10/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.1, max_depth=8;, score=-0.014 total time=   3.8s\n",
      "[CV 1/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.1, max_depth=5;, score=-0.020 total time=   1.6s\n",
      "[CV 2/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.1, max_depth=5;, score=-0.020 total time=   0.6s\n",
      "[CV 3/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.1, max_depth=5;, score=-0.029 total time=   1.8s\n",
      "[CV 4/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.1, max_depth=5;, score=-0.018 total time=   0.5s\n",
      "[CV 5/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.1, max_depth=5;, score=-0.016 total time=   0.5s\n",
      "[CV 6/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.1, max_depth=5;, score=-0.023 total time=   0.5s\n",
      "[CV 7/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.1, max_depth=5;, score=-0.019 total time=   3.1s\n",
      "[CV 8/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.1, max_depth=5;, score=-0.020 total time=   0.9s\n",
      "[CV 9/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.1, max_depth=5;, score=-0.021 total time=   3.0s\n",
      "[CV 10/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.1, max_depth=5;, score=-0.014 total time=   0.8s\n",
      "[CV 1/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.021 total time=   5.3s\n",
      "[CV 2/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.021 total time=   4.4s\n",
      "[CV 3/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.029 total time=   4.5s\n",
      "[CV 4/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.019 total time=   4.7s\n",
      "[CV 5/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.016 total time=   4.7s\n",
      "[CV 6/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.023 total time=   4.6s\n",
      "[CV 7/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.020 total time=   4.2s\n",
      "[CV 8/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.020 total time=   4.1s\n",
      "[CV 9/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.021 total time=   4.4s\n",
      "[CV 10/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.014 total time=   4.3s\n",
      "[CV 1/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.1, max_depth=8;, score=-0.020 total time=   1.1s\n",
      "[CV 2/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.1, max_depth=8;, score=-0.021 total time=   1.1s\n",
      "[CV 3/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.1, max_depth=8;, score=-0.029 total time=   1.6s\n",
      "[CV 4/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.1, max_depth=8;, score=-0.017 total time=   1.2s\n",
      "[CV 5/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.1, max_depth=8;, score=-0.016 total time=   1.3s\n",
      "[CV 6/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.1, max_depth=8;, score=-0.023 total time=   1.9s\n",
      "[CV 7/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.1, max_depth=8;, score=-0.019 total time=   1.5s\n",
      "[CV 8/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.1, max_depth=8;, score=-0.020 total time=   1.2s\n",
      "[CV 9/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.1, max_depth=8;, score=-0.021 total time=   2.0s\n",
      "[CV 10/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.1, max_depth=8;, score=-0.015 total time=   1.6s\n",
      "[CV 1/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.01, max_depth=8;, score=-0.020 total time=   3.4s\n",
      "[CV 2/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.01, max_depth=8;, score=-0.020 total time=   2.5s\n",
      "[CV 3/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.01, max_depth=8;, score=-0.029 total time=   4.5s\n",
      "[CV 4/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.01, max_depth=8;, score=-0.018 total time=   3.6s\n",
      "[CV 5/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.01, max_depth=8;, score=-0.016 total time=   4.2s\n",
      "[CV 6/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.01, max_depth=8;, score=-0.023 total time=   5.3s\n",
      "[CV 7/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.01, max_depth=8;, score=-0.019 total time=   3.5s\n",
      "[CV 8/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.01, max_depth=8;, score=-0.020 total time=   3.3s\n",
      "[CV 9/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.01, max_depth=8;, score=-0.021 total time=   4.2s\n",
      "[CV 10/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.01, max_depth=8;, score=-0.014 total time=   3.1s\n",
      "[CV 1/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.02, max_depth=5;, score=-0.020 total time=   2.1s\n",
      "[CV 2/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.02, max_depth=5;, score=-0.020 total time=   4.0s\n",
      "[CV 3/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.02, max_depth=5;, score=-0.029 total time=   4.6s\n",
      "[CV 4/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.02, max_depth=5;, score=-0.018 total time=   1.7s\n",
      "[CV 5/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.02, max_depth=5;, score=-0.016 total time=   2.1s\n",
      "[CV 6/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.02, max_depth=5;, score=-0.023 total time=   1.9s\n",
      "[CV 7/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.02, max_depth=5;, score=-0.019 total time=   0.8s\n",
      "[CV 8/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.02, max_depth=5;, score=-0.020 total time=   2.1s\n",
      "[CV 9/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.02, max_depth=5;, score=-0.021 total time=   1.8s\n",
      "[CV 10/10] END iterations=100, l2_leaf_reg=20, learning_rate=0.02, max_depth=5;, score=-0.014 total time=   2.6s\n",
      "[CV 1/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.02, max_depth=8;, score=-0.020 total time=   2.9s\n",
      "[CV 2/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.02, max_depth=8;, score=-0.020 total time=   5.5s\n",
      "[CV 3/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.02, max_depth=8;, score=-0.029 total time=   5.6s\n",
      "[CV 4/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.02, max_depth=8;, score=-0.017 total time=   4.4s\n",
      "[CV 5/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.02, max_depth=8;, score=-0.016 total time=   2.4s\n",
      "[CV 6/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.02, max_depth=8;, score=-0.023 total time=   2.2s\n",
      "[CV 7/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.02, max_depth=8;, score=-0.018 total time=   3.2s\n",
      "[CV 8/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.02, max_depth=8;, score=-0.020 total time=   2.0s\n",
      "[CV 9/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.02, max_depth=8;, score=-0.021 total time=   3.1s\n",
      "[CV 10/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.02, max_depth=8;, score=-0.014 total time=   4.8s\n",
      "[CV 1/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.1, max_depth=8;, score=-0.021 total time=   4.8s\n",
      "[CV 2/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.1, max_depth=8;, score=-0.021 total time=   3.3s\n",
      "[CV 3/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.1, max_depth=8;, score=-0.028 total time=   5.2s\n",
      "[CV 4/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.1, max_depth=8;, score=-0.017 total time=   3.3s\n",
      "[CV 5/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.1, max_depth=8;, score=-0.016 total time=   2.5s\n",
      "[CV 6/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.1, max_depth=8;, score=-0.023 total time=   2.8s\n",
      "[CV 7/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.1, max_depth=8;, score=-0.019 total time=   3.4s\n",
      "[CV 8/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.1, max_depth=8;, score=-0.020 total time=   3.8s\n",
      "[CV 9/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.1, max_depth=8;, score=-0.021 total time=   3.8s\n",
      "[CV 10/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.1, max_depth=8;, score=-0.014 total time=   5.1s\n",
      "[CV 1/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.01, max_depth=8;, score=-0.020 total time=   5.0s\n",
      "[CV 2/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.01, max_depth=8;, score=-0.020 total time=   6.4s\n",
      "[CV 3/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.01, max_depth=8;, score=-0.029 total time=   4.8s\n",
      "[CV 4/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.01, max_depth=8;, score=-0.017 total time=   5.4s\n",
      "[CV 5/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.01, max_depth=8;, score=-0.016 total time=   4.2s\n",
      "[CV 6/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.01, max_depth=8;, score=-0.023 total time=   7.0s\n",
      "[CV 7/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.01, max_depth=8;, score=-0.018 total time=   7.3s\n",
      "[CV 8/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.01, max_depth=8;, score=-0.020 total time=   5.5s\n",
      "[CV 9/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.01, max_depth=8;, score=-0.021 total time=   5.0s\n",
      "[CV 10/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.01, max_depth=8;, score=-0.014 total time=   4.4s\n",
      "[CV 1/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.1, max_depth=10;, score=-0.020 total time=   3.1s\n",
      "[CV 2/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.1, max_depth=10;, score=-0.021 total time=   5.5s\n",
      "[CV 3/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.1, max_depth=10;, score=-0.029 total time=   2.9s\n",
      "[CV 4/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.1, max_depth=10;, score=-0.018 total time=   2.2s\n",
      "[CV 5/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.1, max_depth=10;, score=-0.016 total time=   3.7s\n",
      "[CV 6/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.1, max_depth=10;, score=-0.023 total time=   4.5s\n",
      "[CV 7/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.1, max_depth=10;, score=-0.019 total time=   4.1s\n",
      "[CV 8/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.1, max_depth=10;, score=-0.020 total time=   3.3s\n",
      "[CV 9/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.1, max_depth=10;, score=-0.021 total time=   2.3s\n",
      "[CV 10/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.1, max_depth=10;, score=-0.014 total time=   2.2s\n",
      "[CV 1/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=5;, score=-0.020 total time=   0.5s\n",
      "[CV 2/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=5;, score=-0.020 total time=   1.4s\n",
      "[CV 3/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=5;, score=-0.029 total time=   2.5s\n",
      "[CV 4/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=5;, score=-0.018 total time=   0.7s\n",
      "[CV 5/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=5;, score=-0.016 total time=   1.8s\n",
      "[CV 6/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=5;, score=-0.024 total time=   0.6s\n",
      "[CV 7/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=5;, score=-0.019 total time=   1.6s\n",
      "[CV 8/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=5;, score=-0.020 total time=   1.4s\n",
      "[CV 9/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=5;, score=-0.021 total time=   0.8s\n",
      "[CV 10/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=5;, score=-0.014 total time=   1.5s\n",
      "[CV 1/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.01, max_depth=8;, score=-0.020 total time=   3.5s\n",
      "[CV 2/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.01, max_depth=8;, score=-0.020 total time=   3.8s\n",
      "[CV 3/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.01, max_depth=8;, score=-0.029 total time=   3.2s\n",
      "[CV 4/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.01, max_depth=8;, score=-0.018 total time=   3.2s\n",
      "[CV 5/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.01, max_depth=8;, score=-0.016 total time=   3.4s\n",
      "[CV 6/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.01, max_depth=8;, score=-0.023 total time=   2.2s\n",
      "[CV 7/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.01, max_depth=8;, score=-0.019 total time=   2.0s\n",
      "[CV 8/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.01, max_depth=8;, score=-0.020 total time=   3.5s\n",
      "[CV 9/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.01, max_depth=8;, score=-0.021 total time=   2.7s\n",
      "[CV 10/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.01, max_depth=8;, score=-0.014 total time=   3.0s\n",
      "[CV 1/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.02, max_depth=8;, score=-0.020 total time=   5.2s\n",
      "[CV 2/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.02, max_depth=8;, score=-0.020 total time=   7.5s\n",
      "[CV 3/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.02, max_depth=8;, score=-0.028 total time=   6.3s\n",
      "[CV 4/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.02, max_depth=8;, score=-0.017 total time=   5.4s\n",
      "[CV 5/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.02, max_depth=8;, score=-0.016 total time=   5.9s\n",
      "[CV 6/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.02, max_depth=8;, score=-0.023 total time=   4.5s\n",
      "[CV 7/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.02, max_depth=8;, score=-0.018 total time=   3.9s\n",
      "[CV 8/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.02, max_depth=8;, score=-0.020 total time=   3.9s\n",
      "[CV 9/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.02, max_depth=8;, score=-0.021 total time=   3.9s\n",
      "[CV 10/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.02, max_depth=8;, score=-0.014 total time=   5.6s\n",
      "[CV 1/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.1, max_depth=5;, score=-0.022 total time=   2.6s\n",
      "[CV 2/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.1, max_depth=5;, score=-0.021 total time=   3.0s\n",
      "[CV 3/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.1, max_depth=5;, score=-0.028 total time=   2.3s\n",
      "[CV 4/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.1, max_depth=5;, score=-0.017 total time=   2.9s\n",
      "[CV 5/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.1, max_depth=5;, score=-0.016 total time=   1.9s\n",
      "[CV 6/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.1, max_depth=5;, score=-0.023 total time=   2.3s\n",
      "[CV 7/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.1, max_depth=5;, score=-0.019 total time=   2.7s\n",
      "[CV 8/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.1, max_depth=5;, score=-0.020 total time=   1.9s\n",
      "[CV 9/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.1, max_depth=5;, score=-0.021 total time=   4.1s\n",
      "[CV 10/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.1, max_depth=5;, score=-0.014 total time=   2.8s\n",
      "[CV 1/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.1, max_depth=8;, score=-0.020 total time=   1.5s\n",
      "[CV 2/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.1, max_depth=8;, score=-0.020 total time=   0.6s\n",
      "[CV 3/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.1, max_depth=8;, score=-0.029 total time=   1.0s\n",
      "[CV 4/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.1, max_depth=8;, score=-0.017 total time=   0.7s\n",
      "[CV 5/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.1, max_depth=8;, score=-0.016 total time=   0.6s\n",
      "[CV 6/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.1, max_depth=8;, score=-0.023 total time=   3.1s\n",
      "[CV 7/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.1, max_depth=8;, score=-0.018 total time=   0.8s\n",
      "[CV 8/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.1, max_depth=8;, score=-0.020 total time=   0.7s\n",
      "[CV 9/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.1, max_depth=8;, score=-0.021 total time=   1.8s\n",
      "[CV 10/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.1, max_depth=8;, score=-0.014 total time=   0.8s\n",
      "[CV 1/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.022 total time=   1.9s\n",
      "[CV 2/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.021 total time=   4.8s\n",
      "[CV 3/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.029 total time=   3.7s\n",
      "[CV 4/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.018 total time=   1.9s\n",
      "[CV 5/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.016 total time=   2.0s\n",
      "[CV 6/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.023 total time=   2.0s\n",
      "[CV 7/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.019 total time=   1.9s\n",
      "[CV 8/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.020 total time=   1.9s\n",
      "[CV 9/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.021 total time=   2.2s\n",
      "[CV 10/10] END iterations=500, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.014 total time=   2.3s\n",
      "[CV 1/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.020 total time=   0.7s\n",
      "[CV 2/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.020 total time=   0.7s\n",
      "[CV 3/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.029 total time=   0.8s\n",
      "[CV 4/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.018 total time=   0.8s\n",
      "[CV 5/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.016 total time=   1.4s\n",
      "[CV 6/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.023 total time=   0.7s\n",
      "[CV 7/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.019 total time=   0.9s\n",
      "[CV 8/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.020 total time=   1.6s\n",
      "[CV 9/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.021 total time=   1.5s\n",
      "[CV 10/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.014 total time=   0.9s\n",
      "[CV 1/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.020 total time=   1.8s\n",
      "[CV 2/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.020 total time=   1.1s\n",
      "[CV 3/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.029 total time=   1.0s\n",
      "[CV 4/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.018 total time=   2.0s\n",
      "[CV 5/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.016 total time=   1.1s\n",
      "[CV 6/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.023 total time=   1.3s\n",
      "[CV 7/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.019 total time=   1.8s\n",
      "[CV 8/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.020 total time=   1.1s\n",
      "[CV 9/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.021 total time=   1.4s\n",
      "[CV 10/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=8;, score=-0.014 total time=   2.6s\n",
      "[CV 1/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.02, max_depth=10;, score=-0.020 total time=   9.2s\n",
      "[CV 2/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.02, max_depth=10;, score=-0.020 total time=  10.1s\n",
      "[CV 3/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.02, max_depth=10;, score=-0.028 total time=  11.2s\n",
      "[CV 4/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.02, max_depth=10;, score=-0.017 total time=  16.8s\n",
      "[CV 5/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.02, max_depth=10;, score=-0.016 total time=  18.1s\n",
      "[CV 6/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.02, max_depth=10;, score=-0.023 total time=  16.9s\n",
      "[CV 7/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.02, max_depth=10;, score=-0.018 total time=  16.0s\n",
      "[CV 8/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.02, max_depth=10;, score=-0.020 total time=  17.7s\n",
      "[CV 9/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.02, max_depth=10;, score=-0.021 total time=  17.0s\n",
      "[CV 10/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.02, max_depth=10;, score=-0.014 total time=  17.2s\n",
      "[CV 1/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.1, max_depth=10;, score=-0.020 total time=   2.0s\n",
      "[CV 2/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.1, max_depth=10;, score=-0.021 total time=   2.0s\n",
      "[CV 3/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.1, max_depth=10;, score=-0.029 total time=   1.8s\n",
      "[CV 4/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.1, max_depth=10;, score=-0.017 total time=   3.3s\n",
      "[CV 5/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.1, max_depth=10;, score=-0.016 total time=   2.5s\n",
      "[CV 6/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.1, max_depth=10;, score=-0.023 total time=   2.3s\n",
      "[CV 7/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.1, max_depth=10;, score=-0.019 total time=   1.8s\n",
      "[CV 8/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.1, max_depth=10;, score=-0.020 total time=   2.8s\n",
      "[CV 9/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.1, max_depth=10;, score=-0.021 total time=   2.0s\n",
      "[CV 10/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.1, max_depth=10;, score=-0.014 total time=   2.0s\n",
      "[CV 1/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.01, max_depth=10;, score=-0.020 total time=   2.7s\n",
      "[CV 2/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.01, max_depth=10;, score=-0.020 total time=   1.7s\n",
      "[CV 3/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.01, max_depth=10;, score=-0.029 total time=   1.8s\n",
      "[CV 4/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.01, max_depth=10;, score=-0.018 total time=   1.6s\n",
      "[CV 5/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.01, max_depth=10;, score=-0.016 total time=   1.6s\n",
      "[CV 6/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.01, max_depth=10;, score=-0.024 total time=   2.0s\n",
      "[CV 7/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.01, max_depth=10;, score=-0.019 total time=   2.2s\n",
      "[CV 8/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.01, max_depth=10;, score=-0.020 total time=   1.8s\n",
      "[CV 9/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.01, max_depth=10;, score=-0.021 total time=   1.7s\n",
      "[CV 10/10] END iterations=50, l2_leaf_reg=5, learning_rate=0.01, max_depth=10;, score=-0.014 total time=   2.1s\n",
      "[CV 1/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.02, max_depth=5;, score=-0.020 total time=   2.3s\n",
      "[CV 2/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.02, max_depth=5;, score=-0.020 total time=   1.9s\n",
      "[CV 3/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.02, max_depth=5;, score=-0.029 total time=   3.6s\n",
      "[CV 4/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.02, max_depth=5;, score=-0.018 total time=   4.1s\n",
      "[CV 5/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.02, max_depth=5;, score=-0.016 total time=   4.9s\n",
      "[CV 6/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.02, max_depth=5;, score=-0.023 total time=   4.0s\n",
      "[CV 7/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.02, max_depth=5;, score=-0.019 total time=   3.2s\n",
      "[CV 8/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.02, max_depth=5;, score=-0.020 total time=   1.7s\n",
      "[CV 9/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.02, max_depth=5;, score=-0.021 total time=   1.9s\n",
      "[CV 10/10] END iterations=250, l2_leaf_reg=10, learning_rate=0.02, max_depth=5;, score=-0.014 total time=   4.4s\n",
      "[CV 1/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.02, max_depth=10;, score=-0.020 total time=   2.2s\n",
      "[CV 2/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.02, max_depth=10;, score=-0.020 total time=   1.9s\n",
      "[CV 3/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.02, max_depth=10;, score=-0.029 total time=   2.4s\n",
      "[CV 4/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.02, max_depth=10;, score=-0.018 total time=   2.5s\n",
      "[CV 5/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.02, max_depth=10;, score=-0.016 total time=   3.1s\n",
      "[CV 6/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.02, max_depth=10;, score=-0.023 total time=   1.8s\n",
      "[CV 7/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.02, max_depth=10;, score=-0.019 total time=   1.6s\n",
      "[CV 8/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.02, max_depth=10;, score=-0.020 total time=   4.1s\n",
      "[CV 9/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.02, max_depth=10;, score=-0.021 total time=   1.9s\n",
      "[CV 10/10] END iterations=50, l2_leaf_reg=10, learning_rate=0.02, max_depth=10;, score=-0.014 total time=   1.9s\n",
      "[CV 1/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.01, max_depth=5;, score=-0.020 total time=   1.7s\n",
      "[CV 2/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.01, max_depth=5;, score=-0.020 total time=   0.9s\n",
      "[CV 3/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.01, max_depth=5;, score=-0.029 total time=   1.0s\n",
      "[CV 4/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.01, max_depth=5;, score=-0.018 total time=   1.0s\n",
      "[CV 5/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.01, max_depth=5;, score=-0.016 total time=   1.1s\n",
      "[CV 6/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.01, max_depth=5;, score=-0.024 total time=   1.7s\n",
      "[CV 7/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.01, max_depth=5;, score=-0.019 total time=   0.8s\n",
      "[CV 8/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.01, max_depth=5;, score=-0.020 total time=   2.3s\n",
      "[CV 9/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.01, max_depth=5;, score=-0.021 total time=   2.0s\n",
      "[CV 10/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.01, max_depth=5;, score=-0.014 total time=   1.1s\n",
      "[CV 1/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.020 total time=   2.3s\n",
      "[CV 2/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.020 total time=   1.7s\n",
      "[CV 3/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.029 total time=   1.4s\n",
      "[CV 4/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.018 total time=   1.9s\n",
      "[CV 5/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.016 total time=   1.9s\n",
      "[CV 6/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.023 total time=   4.2s\n",
      "[CV 7/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.019 total time=   1.8s\n",
      "[CV 8/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.020 total time=   5.3s\n",
      "[CV 9/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.021 total time=   1.8s\n",
      "[CV 10/10] END iterations=100, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.014 total time=   2.0s\n",
      "[CV 1/10] END iterations=500, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.020 total time=   8.2s\n",
      "[CV 2/10] END iterations=500, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.021 total time=  11.2s\n",
      "[CV 3/10] END iterations=500, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.029 total time=   7.7s\n",
      "[CV 4/10] END iterations=500, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.017 total time=   6.8s\n",
      "[CV 5/10] END iterations=500, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.016 total time=   6.2s\n",
      "[CV 6/10] END iterations=500, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.023 total time=   8.7s\n",
      "[CV 7/10] END iterations=500, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.019 total time=   7.6s\n",
      "[CV 8/10] END iterations=500, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.020 total time=   7.2s\n",
      "[CV 9/10] END iterations=500, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.021 total time=   7.0s\n",
      "[CV 10/10] END iterations=500, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.014 total time=   7.9s\n",
      "[CV 1/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.1, max_depth=5;, score=-0.020 total time=   2.0s\n",
      "[CV 2/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.1, max_depth=5;, score=-0.020 total time=   2.4s\n",
      "[CV 3/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.1, max_depth=5;, score=-0.029 total time=   0.9s\n",
      "[CV 4/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.1, max_depth=5;, score=-0.018 total time=   1.4s\n",
      "[CV 5/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.1, max_depth=5;, score=-0.016 total time=   0.7s\n",
      "[CV 6/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.1, max_depth=5;, score=-0.023 total time=   0.7s\n",
      "[CV 7/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.1, max_depth=5;, score=-0.019 total time=   2.2s\n",
      "[CV 8/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.1, max_depth=5;, score=-0.020 total time=   1.4s\n",
      "[CV 9/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.1, max_depth=5;, score=-0.021 total time=   2.2s\n",
      "[CV 10/10] END iterations=50, l2_leaf_reg=20, learning_rate=0.1, max_depth=5;, score=-0.014 total time=   2.5s\n",
      "[CV 1/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.020 total time=   3.7s\n",
      "[CV 2/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.020 total time=   2.3s\n",
      "[CV 3/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.029 total time=   0.8s\n",
      "[CV 4/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.018 total time=   0.9s\n",
      "[CV 5/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.016 total time=   1.6s\n",
      "[CV 6/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.023 total time=   0.9s\n",
      "[CV 7/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.019 total time=   1.0s\n",
      "[CV 8/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.020 total time=   5.6s\n",
      "[CV 9/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.021 total time=   2.0s\n",
      "[CV 10/10] END iterations=100, l2_leaf_reg=10, learning_rate=0.05, max_depth=5;, score=-0.014 total time=   1.3s\n",
      "[CV 1/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.020 total time=   3.5s\n",
      "[CV 2/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.020 total time=   3.6s\n",
      "[CV 3/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.029 total time=   3.7s\n",
      "[CV 4/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.017 total time=   3.9s\n",
      "[CV 5/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.016 total time=   4.1s\n",
      "[CV 6/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.023 total time=   3.3s\n",
      "[CV 7/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.019 total time=   3.2s\n",
      "[CV 8/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.020 total time=   3.9s\n",
      "[CV 9/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.021 total time=   3.1s\n",
      "[CV 10/10] END iterations=250, l2_leaf_reg=5, learning_rate=0.02, max_depth=8;, score=-0.014 total time=   3.6s\n",
      "[CV 1/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=5;, score=-0.020 total time=   2.4s\n",
      "[CV 2/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=5;, score=-0.020 total time=   1.6s\n",
      "[CV 3/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=5;, score=-0.029 total time=   3.2s\n",
      "[CV 4/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=5;, score=-0.018 total time=   1.6s\n",
      "[CV 5/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=5;, score=-0.016 total time=   1.6s\n",
      "[CV 6/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=5;, score=-0.023 total time=   1.7s\n",
      "[CV 7/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=5;, score=-0.019 total time=   2.3s\n",
      "[CV 8/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=5;, score=-0.020 total time=   2.0s\n",
      "[CV 9/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=5;, score=-0.021 total time=   1.9s\n",
      "[CV 10/10] END iterations=250, l2_leaf_reg=20, learning_rate=0.05, max_depth=5;, score=-0.014 total time=   2.7s\n",
      "[CV 1/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.01, max_depth=5;, score=-0.020 total time=   5.9s\n",
      "[CV 2/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.01, max_depth=5;, score=-0.020 total time=   5.5s\n",
      "[CV 3/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.01, max_depth=5;, score=-0.029 total time=   5.0s\n",
      "[CV 4/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.01, max_depth=5;, score=-0.018 total time=   3.6s\n",
      "[CV 5/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.01, max_depth=5;, score=-0.016 total time=   3.9s\n",
      "[CV 6/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.01, max_depth=5;, score=-0.023 total time=   2.8s\n",
      "[CV 7/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.01, max_depth=5;, score=-0.019 total time=   3.0s\n",
      "[CV 8/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.01, max_depth=5;, score=-0.020 total time=   3.6s\n",
      "[CV 9/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.01, max_depth=5;, score=-0.021 total time=   4.6s\n",
      "[CV 10/10] END iterations=500, l2_leaf_reg=20, learning_rate=0.01, max_depth=5;, score=-0.014 total time=   4.3s\n",
      "24.55791878302892 minutes.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "cbr = CatBoostRegressor(\n",
    "    loss_function='RMSE',\n",
    "    random_seed=29,\n",
    "    od_type='Iter',\n",
    "    od_wait=25,\n",
    "    verbose=0,\n",
    "    )\n",
    "\n",
    "model = hyperparameter_tuning(X_train, X_test, y_train, y_test, cbr, params_CB, cv=5, n_iterations=50)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"{(end-start)/60} minutes.\")\n",
    "best_params_cbr = model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56146d93-7ae6-460d-baaa-1266c7a0dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbr = CatBoostRegressor(\n",
    "    loss_function='RMSE',\n",
    "    random_seed=29,\n",
    "    od_type='Iter',\n",
    "    od_wait=25,\n",
    "    verbose=0,\n",
    "    max_depth=best_params_cbr['max_depth'],\n",
    "    learning_rate=best_params_cbr['learning_rate'],\n",
    "    iterations=best_params_cbr['iterations'],\n",
    "    l2_leaf_reg=best_params_cbr['l2_leaf_reg']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3c72df7-0832-4eff-ab5c-b9b4febf6a22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluated_clusters_cbr = {}\n",
    "\n",
    "for kmns in range(101):\n",
    "    for hier in range(1, 4):\n",
    "        try:\n",
    "            key, = run_evaluate_model(data, cbr, hier, kmns, False).items()\n",
    "            evaluated_clusters_cbr[key[0]] = key[1]\n",
    "        except:\n",
    "            # Some of the clusters only contain last year of sales, so we can skip those\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f201da35-1a1e-4b7d-b944-1b3456012676",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_score_cbr = []\n",
    "for kmns in range(101):\n",
    "        for hier in range(1, 4):\n",
    "            try:\n",
    "                total_score_cbr.append(evaluated_clusters_cbr[f'kmeans-{kmns}_hierarchical-{hier}']['test']['total'])\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a86861d-0f7a-4e6f-8ac3-e30b4e7ada58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11695"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(total_score_cbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "21fafb93-8d87-41ea-a8e8-57cf26a4ffe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0981"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(total_score_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0fff02-9da7-4bd4-aba7-4104e88cd813",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cbr_evaluated.json', 'w') as outfile:\n",
    "    json.dump(evaluated_clusters_cbr, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaa5c01-57df-4814-b738-0b2bd822df47",
   "metadata": {},
   "source": [
    "# AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "24bc5a09-1a28-43ee-9a6e-a981aac41ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_ADA = {'n_estimators':[50,100,500,1000],\n",
    "              'learning_rate':[.001,0.01,.1, .5, 1],\n",
    "              'loss':['square', 'linear'],\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "98c7c943-efe4-4fff-9b7f-9bb160cf56d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 30 candidates, totalling 60 fits\n",
      "[CV 1/2] END learning_rate=0.001, loss=square, n_estimators=50;, score=-0.267 total time=   2.6s\n",
      "[CV 2/2] END learning_rate=0.001, loss=square, n_estimators=50;, score=-0.404 total time=   2.6s\n",
      "[CV 1/2] END learning_rate=0.001, loss=square, n_estimators=100;, score=-0.267 total time=   5.5s\n",
      "[CV 2/2] END learning_rate=0.001, loss=square, n_estimators=100;, score=-0.402 total time=   5.5s\n",
      "[CV 1/2] END learning_rate=0.001, loss=square, n_estimators=500;, score=-0.267 total time=  26.4s\n",
      "[CV 2/2] END learning_rate=0.001, loss=square, n_estimators=500;, score=-0.397 total time=  25.5s\n",
      "[CV 1/2] END learning_rate=0.001, loss=linear, n_estimators=50;, score=-0.268 total time=   2.6s\n",
      "[CV 2/2] END learning_rate=0.001, loss=linear, n_estimators=50;, score=-0.404 total time=   2.7s\n",
      "[CV 1/2] END learning_rate=0.001, loss=linear, n_estimators=100;, score=-0.267 total time=   5.5s\n",
      "[CV 2/2] END learning_rate=0.001, loss=linear, n_estimators=100;, score=-0.402 total time=   5.2s\n",
      "[CV 1/2] END learning_rate=0.001, loss=linear, n_estimators=500;, score=-0.268 total time=  27.5s\n",
      "[CV 2/2] END learning_rate=0.001, loss=linear, n_estimators=500;, score=-0.400 total time=  26.2s\n",
      "[CV 1/2] END learning_rate=0.01, loss=square, n_estimators=50;, score=-0.267 total time=   2.7s\n",
      "[CV 2/2] END learning_rate=0.01, loss=square, n_estimators=50;, score=-0.394 total time=   2.9s\n",
      "[CV 1/2] END learning_rate=0.01, loss=square, n_estimators=100;, score=-0.268 total time=   5.7s\n",
      "[CV 2/2] END learning_rate=0.01, loss=square, n_estimators=100;, score=-0.394 total time=   5.3s\n",
      "[CV 1/2] END learning_rate=0.01, loss=square, n_estimators=500;, score=-0.280 total time=  26.5s\n",
      "[CV 2/2] END learning_rate=0.01, loss=square, n_estimators=500;, score=-0.447 total time=  25.2s\n",
      "[CV 1/2] END learning_rate=0.01, loss=linear, n_estimators=50;, score=-0.268 total time=   2.6s\n",
      "[CV 2/2] END learning_rate=0.01, loss=linear, n_estimators=50;, score=-0.399 total time=   3.0s\n",
      "[CV 1/2] END learning_rate=0.01, loss=linear, n_estimators=100;, score=-0.267 total time=   5.8s\n",
      "[CV 2/2] END learning_rate=0.01, loss=linear, n_estimators=100;, score=-0.399 total time=   5.3s\n",
      "[CV 1/2] END learning_rate=0.01, loss=linear, n_estimators=500;, score=-0.326 total time=  26.8s\n",
      "[CV 2/2] END learning_rate=0.01, loss=linear, n_estimators=500;, score=-0.446 total time=  27.2s\n",
      "[CV 1/2] END learning_rate=0.1, loss=square, n_estimators=50;, score=-0.280 total time=   2.8s\n",
      "[CV 2/2] END learning_rate=0.1, loss=square, n_estimators=50;, score=-0.432 total time=   2.5s\n",
      "[CV 1/2] END learning_rate=0.1, loss=square, n_estimators=100;, score=-0.338 total time=   4.9s\n",
      "[CV 2/2] END learning_rate=0.1, loss=square, n_estimators=100;, score=-0.515 total time=   4.9s\n",
      "[CV 1/2] END learning_rate=0.1, loss=square, n_estimators=500;, score=-1.413 total time=  18.9s\n",
      "[CV 2/2] END learning_rate=0.1, loss=square, n_estimators=500;, score=-0.696 total time=   8.7s\n",
      "[CV 1/2] END learning_rate=0.1, loss=linear, n_estimators=50;, score=-0.312 total time=   2.6s\n",
      "[CV 2/2] END learning_rate=0.1, loss=linear, n_estimators=50;, score=-0.448 total time=   2.6s\n",
      "[CV 1/2] END learning_rate=0.1, loss=linear, n_estimators=100;, score=-0.556 total time=   5.1s\n",
      "[CV 2/2] END learning_rate=0.1, loss=linear, n_estimators=100;, score=-0.552 total time=   5.4s\n",
      "[CV 1/2] END learning_rate=0.1, loss=linear, n_estimators=500;, score=-3.233 total time=  20.7s\n",
      "[CV 2/2] END learning_rate=0.1, loss=linear, n_estimators=500;, score=-0.599 total time=   5.3s\n",
      "[CV 1/2] END learning_rate=0.5, loss=square, n_estimators=50;, score=-0.583 total time=   2.1s\n",
      "[CV 2/2] END learning_rate=0.5, loss=square, n_estimators=50;, score=-0.523 total time=   1.6s\n",
      "[CV 1/2] END learning_rate=0.5, loss=square, n_estimators=100;, score=-0.956 total time=   3.8s\n",
      "[CV 2/2] END learning_rate=0.5, loss=square, n_estimators=100;, score=-0.523 total time=   1.5s\n",
      "[CV 1/2] END learning_rate=0.5, loss=square, n_estimators=500;, score=-4.885 total time=  15.8s\n",
      "[CV 2/2] END learning_rate=0.5, loss=square, n_estimators=500;, score=-0.523 total time=   1.6s\n",
      "[CV 1/2] END learning_rate=0.5, loss=linear, n_estimators=50;, score=-1.373 total time=   2.3s\n",
      "[CV 2/2] END learning_rate=0.5, loss=linear, n_estimators=50;, score=-1.171 total time=   2.4s\n",
      "[CV 1/2] END learning_rate=0.5, loss=linear, n_estimators=100;, score=-2.925 total time=   3.7s\n",
      "[CV 2/2] END learning_rate=0.5, loss=linear, n_estimators=100;, score=-3.939 total time=   4.5s\n",
      "[CV 1/2] END learning_rate=0.5, loss=linear, n_estimators=500;, score=-2.925 total time=   3.5s\n",
      "[CV 2/2] END learning_rate=0.5, loss=linear, n_estimators=500;, score=-5.557 total time=   6.3s\n",
      "[CV 1/2] END learning_rate=1, loss=square, n_estimators=50;, score=-0.991 total time=   1.8s\n",
      "[CV 2/2] END learning_rate=1, loss=square, n_estimators=50;, score=-4.855 total time=   2.0s\n",
      "[CV 1/2] END learning_rate=1, loss=square, n_estimators=100;, score=-1.842 total time=   3.3s\n",
      "[CV 2/2] END learning_rate=1, loss=square, n_estimators=100;, score=-6.634 total time=   2.8s\n",
      "[CV 1/2] END learning_rate=1, loss=square, n_estimators=500;, score=-2.778 total time=  15.0s\n",
      "[CV 2/2] END learning_rate=1, loss=square, n_estimators=500;, score=-6.634 total time=   2.9s\n",
      "[CV 1/2] END learning_rate=1, loss=linear, n_estimators=50;, score=-2.807 total time=   2.2s\n",
      "[CV 2/2] END learning_rate=1, loss=linear, n_estimators=50;, score=-2.375 total time=   2.3s\n",
      "[CV 1/2] END learning_rate=1, loss=linear, n_estimators=100;, score=-3.655 total time=   2.8s\n",
      "[CV 2/2] END learning_rate=1, loss=linear, n_estimators=100;, score=-2.785 total time=   2.4s\n",
      "[CV 1/2] END learning_rate=1, loss=linear, n_estimators=500;, score=-3.655 total time=   2.9s\n",
      "[CV 2/2] END learning_rate=1, loss=linear, n_estimators=500;, score=-2.785 total time=   2.8s\n",
      "7.670264764626821 minutes.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "ada = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "\n",
    "rs = GridSearchCV(\n",
    "            estimator=ada,\n",
    "            param_grid=params_ADA, \n",
    "            cv=2, \n",
    "            n_jobs=1, \n",
    "            scoring='neg_mean_squared_error',\n",
    "            verbose=5\n",
    "        )\n",
    "model = rs.fit(X_train, y_train.quantity_sold.values)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"{(end-start)/60} minutes.\")\n",
    "best_params_ada = model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "02ab83b6-b1e7-443a-8bc1-d8aefc091792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01, 'loss': 'square', 'n_estimators': 50}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d7718d05-2f9f-4636-8d8b-336d66d3eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostRegressor(\n",
    "    random_state=0,\n",
    "    n_estimators=best_params_ada['n_estimators'],\n",
    "    learning_rate=best_params_ada['learning_rate'],\n",
    "    loss=best_params_ada['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "012df52f-56b0-4ec0-96f5-d9fc8c40e496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "evaluated_clusters_ada = {}\n",
    "\n",
    "for kmns in range(101):\n",
    "    print(kmns)\n",
    "    if (kmns == 5):\n",
    "        break\n",
    "    for hier in range(1, 4):\n",
    "        try:\n",
    "            key, = run_evaluate_model(data, ada, hier, kmns, False).items()\n",
    "            evaluated_clusters_ada[key[0]] = key[1]\n",
    "        except:\n",
    "            # Some of the clusters only contain last year of sales, so we can skip those\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96a08bb-2850-48e8-b7d0-e0e4423e07e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_score_ada = []\n",
    "for kmns in range(101):\n",
    "        for hier in range(1, 4):\n",
    "            try:\n",
    "                total_score_ada.append(evaluated_clusters_ada[f'kmeans-{kmns}_hierarchical-{hier}']['test']['total'])\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0643e2-b81c-4e9b-8c0d-ed000aff8039",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b265066-99e0-41f9-ae69-31d3d1ceedba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2761360f-99a4-47dd-a9e3-80f428babff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dccb904-b6f1-4750-831b-12589a9078e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = xgb.predict(X_train)\n",
    "predicted_test = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5e29c6-3dc2-4fca-80c4-4028acb7788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate_model(y_train.quantity_sold.values, predicted))\n",
    "print(evaluate_model(y_test.quantity_sold.values, predicted_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a1cd9e-00e1-42f6-8819-f4b66a121d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pd.DataFrame({'year' : X_train.year, 'week_sin' : X_train.week_sin, 'week_cos' : X_train.week_cos, 'sales' : predicted}).groupby(by=['year', 'week_sin', 'week_cos'], as_index=False).sum().dropna().sales)\n",
    "plt.plot(pd.DataFrame({'year' : X_train.year, 'week_sin' : X_train.week_sin, 'week_cos' : X_train.week_cos, 'sales' : y_train.quantity_sold}).groupby(by=['year', 'week_sin', 'week_cos'], as_index=False).sum().dropna().sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1b12ce-84ac-4f8b-9570-838429e17efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pd.DataFrame({'year' : X_test.year, 'week_sin' : X_test.week_sin, 'week_cos' : X_test.week_cos, 'sales' : predicted_test}).groupby(by=['year', 'week_sin', 'week_cos'], as_index=False).sum().dropna().sales)\n",
    "plt.plot(pd.DataFrame({'year' : X_test.year, 'week_sin' : X_test.week_sin, 'week_cos' : X_test.week_cos, 'sales' : y_test.quantity_sold}).groupby(by=['year', 'week_sin', 'week_cos'], as_index=False).sum().dropna().sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9584942c-e0fe-4a1b-a341-f7cd8370ccc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e87af3-55e1-41e8-86a7-9a7ef03d051d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f525fe6f-6bbd-495a-b134-c12d1b17fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "train = selected_cluster[selected_cluster.year < 2022]\n",
    "test = selected_cluster[selected_cluster.year == 2022]\n",
    "\n",
    "\n",
    "relative_dates = [date(int(train.iloc[i].year*2022), 1, 1) + relativedelta(weeks =+ train.iloc[i].week) for i in range(len(train.index))]\n",
    "relative_dates_test = [date(int(test.iloc[i].year*2022), 1, 1) + relativedelta(weeks =+ test.iloc[i].week) for i in range(len(test.index))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f21f30-fa99-4874-8269-bfe85b5faa52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b69dcd7-d9af-42d8-9dd7-69265f7d1c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5fddef-9880-420b-8b7c-19204f848817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6c255d-cc53-43d5-b83e-3aac014f6802",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO \n",
    "OPIS TRANSOFMEROV MinMax a Cyclical\n",
    "OPIS A UPRATANIE CLUSTERINGU\n",
    "\n",
    "\n",
    "TODO \n",
    "preorbit kolacove grafy na boxploty, bar\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "IDEAS\n",
    "PRIDAJ ROZBITIE NA TRENDY DO Samotneho vyhodnocovania\n",
    "NAPOCITANIE PREDAJOV PRODUKTOV ZA TYZDEN????\n",
    "\n",
    "\n",
    "NEDAVAJ PIE GRAFY DO PRACE!!!!\n",
    "IMPROVEMENT PRICE CHANGE TRACKING - TRACKING OTHER ARGUMENTS AS WELL (ESHOP STOCK COUNT, VISITS, ...)\n",
    "GROUPING ALL INTERNATIONAL SALES\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
